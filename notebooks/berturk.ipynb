{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "791f8cbb-c4e2-4a0b-b2b9-1e3f65bc5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2e9a0-1d86-4eab-bd90-469707ffc1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandarallel.initialize(progress_bar=False, nb_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1111f-49ac-4cd1-85ab-318455fc96d2",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215f85c-0ede-47b2-929f-e61be14809e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/data_cleaned_sentences_2020-04-10.csv\", sep='|', converters={'sentences': pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285dd78-b54b-473d-ab7f-81abdece7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"title\"] = data[\"title\"].parallel_apply(lambda title: title if isinstance(title, str) else \"\") \n",
    "data[\"text\"] = data.parallel_apply(lambda row: \" \".join([sent for sent in [row[\"title\"]] + row[\"sentences\"]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834807b7-7aad-4eac-b292-b6422dce6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index().rename(columns={\"index\": \"id\"})\n",
    "data = data[[\"id\", \"text\", \"Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46bccf6-4239-411e-a588-dca8961c3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = data.loc[19]\n",
    "print(row.title)\n",
    "print(row.sentences)\n",
    "print(\"--------------------\")\n",
    "print(row.text)\n",
    "print(row.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e197d25-219b-40d1-b427-936e854deb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data, open(\"../data/data_id-text-label_2022-10-14.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be644a2-1253-4f6a-96ca-6d099b1ca755",
   "metadata": {},
   "source": [
    "## Huggingface custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adae4e0f-e632-477e-ade0-6e92f65ea47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sahte polislerin kuryesi yakalandı sahte polis...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>kürt üz ama hain değiliz kürtüz ama hain değil...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>suriyeli gelinden altın vurgunu kuyumcuda altı...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>mustafa nevruz sınacı mustafa nevruz sınacı lg...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mustafa nevruz sınacı mustafa nevruz sınacı ya...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25061</th>\n",
       "      <td>25061</td>\n",
       "      <td>amnesty ınternational ve global ahlaksızlık do...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25062</th>\n",
       "      <td>25062</td>\n",
       "      <td>çanakkale asla unutulmamalı llnutturulmamalı ç...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25063</th>\n",
       "      <td>25063</td>\n",
       "      <td>sömürü projesi olarak bop btp genel başkanı pr...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25064</th>\n",
       "      <td>25064</td>\n",
       "      <td>doğruluş zeminimiz helali bir millet istiklali...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25065</th>\n",
       "      <td>25065</td>\n",
       "      <td>yahudilikten islama yönelen bir sahabi abdulla...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25066 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text Label\n",
       "0          0  sahte polislerin kuryesi yakalandı sahte polis...  hate\n",
       "1          1  kürt üz ama hain değiliz kürtüz ama hain değil...  hate\n",
       "2          2  suriyeli gelinden altın vurgunu kuyumcuda altı...  hate\n",
       "3          3  mustafa nevruz sınacı mustafa nevruz sınacı lg...  hate\n",
       "4          4  mustafa nevruz sınacı mustafa nevruz sınacı ya...  hate\n",
       "...      ...                                                ...   ...\n",
       "25061  25061  amnesty ınternational ve global ahlaksızlık do...  hate\n",
       "25062  25062  çanakkale asla unutulmamalı llnutturulmamalı ç...  hate\n",
       "25063  25063  sömürü projesi olarak bop btp genel başkanı pr...  hate\n",
       "25064  25064  doğruluş zeminimiz helali bir millet istiklali...  hate\n",
       "25065  25065  yahudilikten islama yönelen bir sahabi abdulla...  hate\n",
       "\n",
       "[25066 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pickle.load(open(\"../data/data_id-text-label_2022-10-14.pkl\", \"rb\"))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8cf8e2f-c6ff-4d12-9164-ed5dcd5c3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04afaee0-c305-4900-b852-6c101eb53c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDVDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, idxs):\n",
    "        self.label_encodings = {\"not_hate\": 0, \"hate\": 1}\n",
    "        self.encodings = encodings\n",
    "        self.labels = [self.label_encodings[label] for label in labels]\n",
    "        self.idxs = self.idxs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b7fea18-7ba2-4ae9-9d2b-f1e53683eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs, texts, labels = list(data[\"id\"].values), list(data[\"text\"].values), list(data[\"Label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7064ec9e-d0a7-40c3-b217-0d989ebcc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, stratify=labels, test_size=.2, shuffle=True, random_state=17)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, stratify=val_labels, test_size=.5, shuffle=True, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a27b581-5130-4146-98c8-5ff66c8dea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dist: Counter({'hate': 10107, 'not_hate': 9945})\n",
      "Validation dist: Counter({'hate': 1264, 'not_hate': 1243})\n",
      "Test dist: Counter({'hate': 1263, 'not_hate': 1244})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dist: {Counter(train_labels)}\")\n",
    "print(f\"Validation dist: {Counter(val_labels)}\")\n",
    "print(f\"Test dist: {Counter(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8afb469-387c-483a-8501-0212ec785468",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d81c32-7f3e-40d9-8760-4e552000f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f16f64-359d-4a7c-a092-141800b08a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HDVDataset(train_encodings, train_labels)\n",
    "val_dataset = HDVDataset(val_encodings, val_labels)\n",
    "test_dataset = HDVDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c599074-ae92-44fb-8af5-fe40c98255f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "99600c4f-63ba-490e-a4bc-4824caeb784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = load_metric(\"precision\")\n",
    "rec = load_metric(\"recall\")\n",
    "acc = load_metric(\"accuracy\")\n",
    "f1 = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    result = {}\n",
    "    for mtrc in [prec, rec, acc, f1]:\n",
    "        mtrc_result = mtrc.compute(predictions=predictions, references=labels)\n",
    "        result.update(mtrc_result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b0cc9-5464-4f6e-9350-9762310187cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Huggingface models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0d19972-4ecd-4b3d-9630-b69ea9a70f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=hdv_hate_speech\n",
      "env: WANDB_LOG_MODEL=true\n",
      "env: WANDB_WATCH=all\n",
      "env: WANDB_NOTEBOOK_NAME=berturk\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=hdv_hate_speech\n",
    "%env WANDB_LOG_MODEL=true\n",
    "%env WANDB_WATCH=all\n",
    "%env WANDB_NOTEBOOK_NAME=berturk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b920fcab-f99b-4000-8550-ed709ee3f11a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72bd936e-59fa-4392-bda8-1f9283888e45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find berturk.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnlpboun\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36d9fba1-cc80-4d54-9fcc-2a484dccfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"../experiments/results/\"\n",
    "logs_path = \"../experiments/logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc90eea2-815f-41d6-8952-ce0ad236515d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94fd1d23-3c14-4fcd-8668-28a3fc8b7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(results_path, \"berturk_128K\"),               # output directory\n",
    "    num_train_epochs=2,                                                  # total number of training epochs\n",
    "    per_device_train_batch_size=4,                                       # batch size per device during training\n",
    "    per_device_eval_batch_size=4,                                        # batch size for evaluation\n",
    "    warmup_steps=500,                                                    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                                                   # strength of weight decay\n",
    "    logging_dir=os.path.join(results_path, \"berturk_128K\"),              # directory for storing logs\n",
    "    logging_steps=20,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    save_steps=1000,\n",
    "    learning_rate=1e-05,\n",
    "    report_to='wandb',\n",
    "    run_name=\"berturk_128K_uncased_lre-5\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                                                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                                                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,                                         # training dataset\n",
    "    eval_dataset=val_dataset,                                            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c410cbd2-5664-4938-a97e-bf879f92293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tabilab/anaconda3/envs/hate_env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 20052\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10026\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tabilab/projects/notebooks/wandb/run-20220414_135711-3vbi04su</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlpboun/hdv_hate_speech/runs/3vbi04su\" target=\"_blank\">berturk_128K_uncased_lre-5</a></strong> to <a href=\"https://wandb.ai/nlpboun/hdv_hate_speech\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10026' max='10026' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10026/10026 29:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.470382</td>\n",
       "      <td>0.742204</td>\n",
       "      <td>0.847310</td>\n",
       "      <td>0.774631</td>\n",
       "      <td>0.791282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.412100</td>\n",
       "      <td>0.487355</td>\n",
       "      <td>0.869938</td>\n",
       "      <td>0.883703</td>\n",
       "      <td>0.874751</td>\n",
       "      <td>0.876766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.880620</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>0.887515</td>\n",
       "      <td>0.889585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.537385</td>\n",
       "      <td>0.930530</td>\n",
       "      <td>0.805380</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>0.863444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.674700</td>\n",
       "      <td>0.489855</td>\n",
       "      <td>0.929856</td>\n",
       "      <td>0.818038</td>\n",
       "      <td>0.877144</td>\n",
       "      <td>0.870370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.378214</td>\n",
       "      <td>0.902478</td>\n",
       "      <td>0.893196</td>\n",
       "      <td>0.897487</td>\n",
       "      <td>0.897813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.367395</td>\n",
       "      <td>0.888298</td>\n",
       "      <td>0.924842</td>\n",
       "      <td>0.903470</td>\n",
       "      <td>0.906202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.372100</td>\n",
       "      <td>0.365355</td>\n",
       "      <td>0.894453</td>\n",
       "      <td>0.918513</td>\n",
       "      <td>0.904268</td>\n",
       "      <td>0.906323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.552500</td>\n",
       "      <td>0.460086</td>\n",
       "      <td>0.850035</td>\n",
       "      <td>0.959652</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>0.901524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.378452</td>\n",
       "      <td>0.916599</td>\n",
       "      <td>0.895570</td>\n",
       "      <td>0.906262</td>\n",
       "      <td>0.905962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.355999</td>\n",
       "      <td>0.912837</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.911448</td>\n",
       "      <td>0.912114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.388397</td>\n",
       "      <td>0.903502</td>\n",
       "      <td>0.918513</td>\n",
       "      <td>0.909454</td>\n",
       "      <td>0.910945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.430900</td>\n",
       "      <td>0.378103</td>\n",
       "      <td>0.898006</td>\n",
       "      <td>0.926424</td>\n",
       "      <td>0.909852</td>\n",
       "      <td>0.911994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.479300</td>\n",
       "      <td>0.377564</td>\n",
       "      <td>0.918285</td>\n",
       "      <td>0.897943</td>\n",
       "      <td>0.908257</td>\n",
       "      <td>0.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.251800</td>\n",
       "      <td>0.366917</td>\n",
       "      <td>0.887976</td>\n",
       "      <td>0.940665</td>\n",
       "      <td>0.910251</td>\n",
       "      <td>0.913561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.256600</td>\n",
       "      <td>0.377305</td>\n",
       "      <td>0.883309</td>\n",
       "      <td>0.946203</td>\n",
       "      <td>0.909852</td>\n",
       "      <td>0.913675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.361631</td>\n",
       "      <td>0.908949</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.915038</td>\n",
       "      <td>0.916438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.391033</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.925633</td>\n",
       "      <td>0.912246</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.371968</td>\n",
       "      <td>0.913590</td>\n",
       "      <td>0.920095</td>\n",
       "      <td>0.915836</td>\n",
       "      <td>0.916831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.367300</td>\n",
       "      <td>0.364992</td>\n",
       "      <td>0.901602</td>\n",
       "      <td>0.935127</td>\n",
       "      <td>0.915836</td>\n",
       "      <td>0.918058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-1000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-1000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-2000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-2000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-3000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-3000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-4000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-4000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-4000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-5000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-5000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-5000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-6000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-6000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-6000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-7000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-7000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-7000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-8000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-8000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-8000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-9000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-9000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-9000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-10000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-10000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-10000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../experiments/results/berturk_128K/checkpoint-10000 (score: 0.3649916350841522).\n",
      "Saving model checkpoint to /tmp/tmpu3xrqjht\n",
      "Configuration saved in /tmp/tmpu3xrqjht/config.json\n",
      "Model weights saved in /tmp/tmpu3xrqjht/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10026, training_loss=0.38614428408388934, metrics={'train_runtime': 1770.0672, 'train_samples_per_second': 22.657, 'train_steps_per_second': 5.664, 'total_flos': 1.055180576415744e+16, 'train_loss': 0.38614428408388934, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad39b6-4966-4c95-aec6-669ed22de961",
   "metadata": {},
   "source": [
    "## Evaluation (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3110b46c-0a69-4358-9ddf-380376b94000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2642' max='627' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [627/627 35:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3649916350841522,\n",
       " 'eval_precision': 0.9016018306636155,\n",
       " 'eval_recall': 0.935126582278481,\n",
       " 'eval_accuracy': 0.9158356601515756,\n",
       " 'eval_f1': 0.9180582524271845,\n",
       " 'eval_runtime': 22.3554,\n",
       " 'eval_samples_per_second': 112.143,\n",
       " 'eval_steps_per_second': 28.047,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = trainer.evaluate(val_dataset)\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8703b-9124-4c17-b393-218a9faa0524",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20e05409-016d-43bb-b5cc-ad44ec3cace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDVDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts_idxs, labels, tokenizer):\n",
    "        self.label_encodings = {\"not_hate\": 0, \"hate\": 1}\n",
    "        self.rev_label_encodings = {0: \"not_hate\", 1: \"hate\"}\n",
    "        \n",
    "        self.texts, self.idxs = list(np.array(texts_idxs)[:, 0]), list(np.array(texts_idxs)[:, 1])\n",
    "        self.encodings = tokenizer(self.texts, truncation=True, padding=True)\n",
    "        self.labels = [self.label_encodings[label] for label in labels]\n",
    "        self.preds = []\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _get_preds_with_idx(self):\n",
    "        df_preds = pd.DataFrame(data={\"idx\": self.idxs, \"prediction\": self.preds})\n",
    "        df_preds[\"prediction\"] = df_preds[\"prediction\"].map(self.rev_label_encodings)\n",
    "        return df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b33ce1fa-1894-4280-8ce7-960e0da33ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_idxs, labels = list(data[[\"text\", \"id\"]].values), list(data[\"Label\"].values)\n",
    "train_texts_idxs_2, val_texts_idxs_2, train_labels_2, val_labels_2 = train_test_split(texts_idxs, labels, stratify=labels, test_size=.2, shuffle=True, random_state=17)\n",
    "val_texts_idxs_2, test_texts_idxs_2, val_labels_2, test_labels_2 = train_test_split(val_texts_idxs_2, val_labels_2, stratify=val_labels_2, test_size=.5, shuffle=True, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f944246f-86a6-4010-966d-08e99938b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_idx = HDVDatasetTest(test_texts_idxs_2, test_labels_2, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa51f1-9524-47db-8249-b6d24dddccdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7aa375aa-f4f4-4478-9b29-f0f4e68c527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1 0 1 ... 1 0 1]\n",
      " GT's: [1 0 1 ... 1 0 1]\n",
      "{'test_loss': 0.42105528712272644, 'test_precision': 0.8891419893697798, 'test_recall': 0.9271575613618369, 'test_accuracy': 0.9050658157159952, 'test_f1': 0.9077519379844963, 'test_runtime': 23.0777, 'test_samples_per_second': 108.633, 'test_steps_per_second': 27.169}\n"
     ]
    }
   ],
   "source": [
    "preds_dict = trainer.predict(test_dataset)\n",
    "predictions = preds_dict.predictions\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print(f\"Preds: {predictions}\\n GT's: {preds_dict.label_ids}\")\n",
    "print(preds_dict.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "209c979d-16db-49e2-b5fa-97f4e345ca5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37011e44-0fdd-492e-9ad8-0ca38221b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1 0 1 ... 1 0 1]\n",
      " GT's: [1 0 1 ... 1 0 1]\n",
      "{'test_loss': 0.42105528712272644, 'test_precision': 0.8891419893697798, 'test_recall': 0.9271575613618369, 'test_accuracy': 0.9050658157159952, 'test_f1': 0.9077519379844963, 'test_runtime': 23.2055, 'test_samples_per_second': 108.035, 'test_steps_per_second': 27.019}\n"
     ]
    }
   ],
   "source": [
    "preds_dict_2 = trainer.predict(test_dataset_idx)\n",
    "predictions_2 = preds_dict_2.predictions\n",
    "predictions_2 = np.argmax(predictions_2, axis=1)\n",
    "print(f\"Preds: {predictions_2}\\n GT's: {preds_dict_2.label_ids}\")\n",
    "print(preds_dict_2.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2585239a-7278-4a05-ad6d-5c79efe0fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_idx.preds = predictions_2\n",
    "df_preds = test_dataset_idx._get_preds_with_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d82d23b7-f456-4230-b3cd-79d07986bcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3973</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16849</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9186</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3072</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10346</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>3672</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>18446</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>3657</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>19535</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>8945</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2507 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx prediction\n",
       "0      3973       hate\n",
       "1     16849   not_hate\n",
       "2      9186       hate\n",
       "3      3072       hate\n",
       "4     10346       hate\n",
       "...     ...        ...\n",
       "2502   3672       hate\n",
       "2503  18446   not_hate\n",
       "2504   3657       hate\n",
       "2505  19535   not_hate\n",
       "2506   8945       hate\n",
       "\n",
       "[2507 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab619e57-c533-480e-bc2a-c32d0f174665",
   "metadata": {},
   "source": [
    "## Report to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbd5a102-9b0d-45be-9335-3fa46f999bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_preds = pd.merge(data, df_preds, left_on=\"id\", right_on=\"idx\", how=\"right\").drop(\"idx\", axis=1)\n",
    "df_label_preds.to_excel(\"../outputs/labels_preds_berturk_2022-04-14.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce0d0553-4d65-455f-bd9a-41a2c41b013a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3973</td>\n",
       "      <td>haber seyfullah koyuncu freddy mercurynin aske...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16849</td>\n",
       "      <td>kilisede hz fatıma nın doğumu kutlandı kilised...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9186</td>\n",
       "      <td>yunanlıların verdiği zararları anlatan resmi d...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3072</td>\n",
       "      <td>itı hr it serdar çalışkan yaptığı açıklamada ş...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10346</td>\n",
       "      <td>mültecileri dövüp geri gönderdiler mültecileri...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>3672</td>\n",
       "      <td>saitiyor su rl yıu ur bpfjmkmmiami rj fiil il ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>18446</td>\n",
       "      <td>bu işbirllfii türkiye ye örnek oucak bu işbirl...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>3657</td>\n",
       "      <td>mersinde işlenen cinayetle ilgili suriyeli tut...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>19535</td>\n",
       "      <td>siparişle kurulan proje örgütlerdir siparişle ...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>8945</td>\n",
       "      <td>içimizi içimizi döktüğümüz duvarlar handan yal...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2507 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text     Label  \\\n",
       "0      3973  haber seyfullah koyuncu freddy mercurynin aske...      hate   \n",
       "1     16849  kilisede hz fatıma nın doğumu kutlandı kilised...  not_hate   \n",
       "2      9186  yunanlıların verdiği zararları anlatan resmi d...      hate   \n",
       "3      3072  itı hr it serdar çalışkan yaptığı açıklamada ş...      hate   \n",
       "4     10346  mültecileri dövüp geri gönderdiler mültecileri...      hate   \n",
       "...     ...                                                ...       ...   \n",
       "2502   3672  saitiyor su rl yıu ur bpfjmkmmiami rj fiil il ...      hate   \n",
       "2503  18446  bu işbirllfii türkiye ye örnek oucak bu işbirl...  not_hate   \n",
       "2504   3657  mersinde işlenen cinayetle ilgili suriyeli tut...      hate   \n",
       "2505  19535  siparişle kurulan proje örgütlerdir siparişle ...  not_hate   \n",
       "2506   8945  içimizi içimizi döktüğümüz duvarlar handan yal...      hate   \n",
       "\n",
       "     prediction  \n",
       "0          hate  \n",
       "1      not_hate  \n",
       "2          hate  \n",
       "3          hate  \n",
       "4          hate  \n",
       "...         ...  \n",
       "2502       hate  \n",
       "2503   not_hate  \n",
       "2504       hate  \n",
       "2505   not_hate  \n",
       "2506       hate  \n",
       "\n",
       "[2507 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a81a4df-ebec-4a0c-b12e-dee52fdd9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"../data/data_cleaned_sentences_2020-04-10.csv\", sep='|', converters={'sentences': pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ec565bc-ab4f-4756-ab2e-4d78bc1bb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = data_raw.reset_index().rename(columns={\"index\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c3c1a31-0846-4d15-9eb9-e902eeae20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_and_preds = pd.merge(data_raw, df_preds, left_on=\"id\", right_on=\"idx\", how=\"right\").drop(\"idx\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d26d628d-4c80-4ce2-87d3-e531957336a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_and_preds.to_excel(\"../outputs/data_and_preds_2022-04-14.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9e89c12-ec6d-4715-af9b-f77f0db75658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9050658157159952"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_data_and_preds[\"Label\"] == df_data_and_preds[\"prediction\"]) / df_data_and_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c4ff115a-25c0-41cd-9a99-9eb2b5c187c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hate': 1264, 'not_hate': 1243})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828fbda-52e1-47aa-a037-495319d60750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea187edb-7a5d-4522-ada6-506674386429",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../outputs/data_and_preds_2022-04-14.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0036d72-5356-426b-b367-3a6202140069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9046856227472968"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(df[\"Label\"] == df[\"prediction\"]) - len(remove_id_from_test)) / (df.shape[0] - len(remove_id_from_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d46c4-323b-4151-b093-920fc9a7609f",
   "metadata": {},
   "source": [
    "## Duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291e06b8-5b3a-4d4c-be19-8c0a104972f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_excel = pd.read_excel(\"../outputs/data_and_preds_2022-04-14.xlsx\")[\"id\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16084c1-14aa-4787-b137-3fcffd686f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.array(test_texts_idxs_2)[:, 1]) == id_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5234732-8ef7-4d34-9041-564f3570a220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set.intersection(set(np.array(test_texts_idxs_2)[:, 1]), set(np.array(train_texts_idxs_2)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f10f8-ebdf-47a0-b5c3-2eef44886513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8eafbcf3-f78e-430a-abaf-bced5e0f70d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pair = data[data[[\"text\"]].duplicated(keep=\"first\")].sort_values(\"text\").id.values.tolist()\n",
    "last_pair = data[data[[\"text\"]].duplicated(keep=\"last\")].sort_values(\"text\").id.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7831696a-0bd6-477a-998a-43b6d53b193a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15058, 2247],\n",
       " [886, 855],\n",
       " [1658, 1653],\n",
       " [270, 269],\n",
       " [8027, 8026],\n",
       " [888, 887],\n",
       " [3733, 3732],\n",
       " [7759, 7758],\n",
       " [1968, 1967],\n",
       " [6104, 6095],\n",
       " [7982, 7861],\n",
       " [11343, 2715],\n",
       " [9919, 9705],\n",
       " [951, 933],\n",
       " [3124, 3117],\n",
       " [11376, 11368],\n",
       " [19910, 7489],\n",
       " [10388, 10130],\n",
       " [998, 997],\n",
       " [8034, 7955],\n",
       " [2530, 2529],\n",
       " [554, 545],\n",
       " [8926, 8925],\n",
       " [22303, 22298],\n",
       " [4438, 4432],\n",
       " [6020, 6015],\n",
       " [860, 828],\n",
       " [1243, 1141],\n",
       " [1164, 1163],\n",
       " [973, 972],\n",
       " [11053, 9867],\n",
       " [6744, 6400],\n",
       " [7632, 7631],\n",
       " [19117, 6778],\n",
       " [3607, 3590],\n",
       " [1054, 1011],\n",
       " [11955, 11945],\n",
       " [24835, 24800],\n",
       " [2110, 2109],\n",
       " [3218, 3217],\n",
       " [11342, 2714],\n",
       " [557, 539],\n",
       " [5744, 5742],\n",
       " [21625, 21619],\n",
       " [7201, 7179],\n",
       " [1804, 1788],\n",
       " [12017, 12002],\n",
       " [7984, 7862],\n",
       " [11374, 11367],\n",
       " [16850, 3637],\n",
       " [2449, 2448]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [[first_pair[i], last_pair[i]] for i in range(len(first_pair))]\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99e7e8fc-a3b0-4d8d-9078-33117af2fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, val_idxs, test_idxs = list(np.array(train_texts_idxs_2)[:, 1]), list(np.array(val_texts_idxs_2)[:, 1]), list(np.array(test_texts_idxs_2)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22b1a03d-ff28-4c96-ae68-eb46c7a7de37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_duplicate_rows(train_idxs, test_idxs, pairs):\n",
    "    remove_id_from_test = []\n",
    "    for pair in pairs:\n",
    "        if pair[0] in train_idxs and pair[1] in train_idxs:\n",
    "            print(pair, \" is all in train set!\")\n",
    "        elif pair[0] in train_idxs and pair[1] in test_idxs:\n",
    "            print(f\"{pair[0]} in train, {pair[1]} in test, please delete from test!!!\")\n",
    "            remove_id_from_test.append(pair[1])\n",
    "        elif pair[1] in train_idxs and pair[0] in test_idxs:\n",
    "            print(f\"{pair[1]} in train, {pair[0]} in test, please delete from test!!!\")\n",
    "            remove_id_from_test.append(pair[0])\n",
    "        elif pair[0] in test_idxs and pair[1] in test_idxs:\n",
    "            print(f\"{pair[0]} in test, {pair[1]} in test, please delete one of them from test!!!\")\n",
    "            remove_id_from_test.append(pair[1])\n",
    "    remove_test_indices = []\n",
    "    for remove_id in remove_id_from_test:\n",
    "        remove_test_indices.append(test_idxs.index(remove_id))\n",
    "    remove_test_indices\n",
    "    return remove_id_from_test, remove_test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e138719f-89c6-47e0-9b25-9d34c9c61493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15058, 2247]  is all in train set!\n",
      "[886, 855]  is all in train set!\n",
      "[1658, 1653]  is all in train set!\n",
      "[270, 269]  is all in train set!\n",
      "[8027, 8026]  is all in train set!\n",
      "888 in train, 887 in test, please delete from test!!!\n",
      "[7759, 7758]  is all in train set!\n",
      "6104 in train, 6095 in test, please delete from test!!!\n",
      "[11343, 2715]  is all in train set!\n",
      "[9919, 9705]  is all in train set!\n",
      "[951, 933]  is all in train set!\n",
      "[3124, 3117]  is all in train set!\n",
      "[11376, 11368]  is all in train set!\n",
      "[10388, 10130]  is all in train set!\n",
      "[998, 997]  is all in train set!\n",
      "[8034, 7955]  is all in train set!\n",
      "[2530, 2529]  is all in train set!\n",
      "545 in train, 554 in test, please delete from test!!!\n",
      "[8926, 8925]  is all in train set!\n",
      "22303 in train, 22298 in test, please delete from test!!!\n",
      "[4438, 4432]  is all in train set!\n",
      "[6020, 6015]  is all in train set!\n",
      "[860, 828]  is all in train set!\n",
      "[1243, 1141]  is all in train set!\n",
      "[1164, 1163]  is all in train set!\n",
      "[973, 972]  is all in train set!\n",
      "[11053, 9867]  is all in train set!\n",
      "[6744, 6400]  is all in train set!\n",
      "[7632, 7631]  is all in train set!\n",
      "[3607, 3590]  is all in train set!\n",
      "1054 in train, 1011 in test, please delete from test!!!\n",
      "[11955, 11945]  is all in train set!\n",
      "[24835, 24800]  is all in train set!\n",
      "[2110, 2109]  is all in train set!\n",
      "3218 in test, 3217 in test, please delete one of them from test!!!\n",
      "[11342, 2714]  is all in train set!\n",
      "21625 in test, 21619 in test, please delete one of them from test!!!\n",
      "7179 in train, 7201 in test, please delete from test!!!\n",
      "1804 in test, 1788 in test, please delete one of them from test!!!\n",
      "[12017, 12002]  is all in train set!\n",
      "[7984, 7862]  is all in train set!\n",
      "[11374, 11367]  is all in train set!\n",
      "16850 in test, 3637 in test, please delete one of them from test!!!\n",
      "[2449, 2448]  is all in train set!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[887, 6095, 554, 22298, 1011, 3217, 21619, 7201, 1788, 3637]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_id_from_test = detect_duplicate_rows(train_idxs, test_idxs, pairs)\n",
    "remove_id_from_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4ab89b3-c3ad-412b-bedf-d5b497cb6df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15058, 2247]  is all in train set!\n",
      "[886, 855]  is all in train set!\n",
      "[1658, 1653]  is all in train set!\n",
      "[270, 269]  is all in train set!\n",
      "[8027, 8026]  is all in train set!\n",
      "3732 in train, 3733 in test, please delete from test!!!\n",
      "[7759, 7758]  is all in train set!\n",
      "1967 in train, 1968 in test, please delete from test!!!\n",
      "[11343, 2715]  is all in train set!\n",
      "[9919, 9705]  is all in train set!\n",
      "[951, 933]  is all in train set!\n",
      "[3124, 3117]  is all in train set!\n",
      "[11376, 11368]  is all in train set!\n",
      "19910 in train, 7489 in test, please delete from test!!!\n",
      "[10388, 10130]  is all in train set!\n",
      "[998, 997]  is all in train set!\n",
      "[8034, 7955]  is all in train set!\n",
      "[2530, 2529]  is all in train set!\n",
      "[8926, 8925]  is all in train set!\n",
      "[4438, 4432]  is all in train set!\n",
      "[6020, 6015]  is all in train set!\n",
      "[860, 828]  is all in train set!\n",
      "[1243, 1141]  is all in train set!\n",
      "[1164, 1163]  is all in train set!\n",
      "[973, 972]  is all in train set!\n",
      "[11053, 9867]  is all in train set!\n",
      "[6744, 6400]  is all in train set!\n",
      "[7632, 7631]  is all in train set!\n",
      "19117 in train, 6778 in test, please delete from test!!!\n",
      "[3607, 3590]  is all in train set!\n",
      "[11955, 11945]  is all in train set!\n",
      "[24835, 24800]  is all in train set!\n",
      "[2110, 2109]  is all in train set!\n",
      "[11342, 2714]  is all in train set!\n",
      "557 in train, 539 in test, please delete from test!!!\n",
      "5744 in train, 5742 in test, please delete from test!!!\n",
      "[12017, 12002]  is all in train set!\n",
      "[7984, 7862]  is all in train set!\n",
      "[11374, 11367]  is all in train set!\n",
      "[2449, 2448]  is all in train set!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3733, 1968, 7489, 6778, 539, 5742]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_id_from_val = detect_duplicate_rows(train_idxs, val_idxs, pairs)\n",
    "remove_id_from_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5fd6ab-6007-43db-8423-10ac8af732fe",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa3e05c3-543c-439c-b544-ac099672bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"../experiments/results/berturk_128K/checkpoint-10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52c3c2d3-765e-4610-98df-7f4683585bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDVDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts_idxs, labels, tokenizer, remove_idxs):\n",
    "        self.label_encodings = {\"not_hate\": 0, \"hate\": 1}\n",
    "        self.rev_label_encodings = {0: \"not_hate\", 1: \"hate\"}\n",
    "        \n",
    "        self.texts, self.idxs = list(np.array(texts_idxs)[:, 0]), list(np.array(texts_idxs)[:, 1])\n",
    "        print(len(self.idxs))\n",
    "        self.texts = [text for i, text in enumerate(self.texts) if i not in remove_idxs]\n",
    "        self.idxs = [idx for i, idx in enumerate(self.idxs) if i not in remove_idxs]\n",
    "        self.encodings = tokenizer(self.texts, truncation=True, padding=True)\n",
    "        self.labels = [self.label_encodings[label] for i, label in enumerate(labels) if i not in remove_idxs]\n",
    "        self.preds = []\n",
    "        print(len(self.idxs))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _get_preds_with_idx(self):\n",
    "        df_preds = pd.DataFrame(data={\"idx\": self.idxs, \"prediction\": self.preds})\n",
    "        df_preds[\"prediction\"] = df_preds[\"prediction\"].map(self.rev_label_encodings)\n",
    "        return df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3df4d9c6-04e3-430f-9b02-a2e6c5e21349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2507\n",
      "2497\n"
     ]
    }
   ],
   "source": [
    "test_dataset_idx_cleaned = HDVDatasetTest(test_texts_idxs_2, test_labels_2, tokenizer, remove_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e4c2db2-0a4a-4865-9981-fc789fcb3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                                                         # the instantiated 🤗 Transformers model to be trained\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "542b9347-3921-44e9-8cab-ed99cb243699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2497\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1 0 1 ... 1 0 1]\n",
      " GT's: [1 0 1 ... 1 0 1]\n",
      "{'test_loss': 0.4224632978439331, 'test_precision': 0.8884644766997708, 'test_recall': 0.9266932270916335, 'test_accuracy': 0.9046856227472968, 'test_f1': 0.9071762870514819, 'test_runtime': 17.3161, 'test_samples_per_second': 144.201, 'test_steps_per_second': 18.076}\n"
     ]
    }
   ],
   "source": [
    "preds_dict_3 = trainer.predict(test_dataset_idx_cleaned)\n",
    "predictions_3 = preds_dict_3.predictions\n",
    "predictions_3 = np.argmax(predictions_3, axis=1)\n",
    "print(f\"Preds: {predictions_3}\\n GT's: {preds_dict_3.label_ids}\")\n",
    "print(preds_dict_3.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf03f58-29f3-4fd2-9000-a3b62809d059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hate env",
   "language": "python",
   "name": "hate_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
