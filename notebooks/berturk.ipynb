{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "791f8cbb-c4e2-4a0b-b2b9-1e3f65bc5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db2e9a0-1d86-4eab-bd90-469707ffc1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar=False, nb_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1111f-49ac-4cd1-85ab-318455fc96d2",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b215f85c-0ede-47b2-929f-e61be14809e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../data/data_cleaned_sentences_phases_2020-04-16.csv\", sep='|', converters={'sentences': pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e285dd78-b54b-473d-ab7f-81abdece7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"title\"] = data[\"title\"].parallel_apply(lambda title: title if isinstance(title, str) else \"\") \n",
    "data[\"text\"] = data.parallel_apply(lambda row: \" \".join([sent for sent in [row[\"title\"]] + row[\"sentences\"]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834807b7-7aad-4eac-b292-b6422dce6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index().rename(columns={\"index\": \"id\"})\n",
    "data = data[[\"id\", \"text\", \"Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46bccf6-4239-411e-a588-dca8961c3390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haber\n",
      "['filistinde yahudi yerleÅŸimcilerin polis korumasÄ±nda mescidi aksanÄ±n avlusuna girmesi gerginliÄŸe neden oldu', 'yahudi yerleÅŸimcilerden oluÅŸan kiÅŸilik grup israil polisinin korumasÄ± altÄ±nda mescidi aksanÄ±n avlusuna girdi', 'bunun Ã¼zerine birgrup filistinli gÃ¶nÃ¼llÃ¼ kadÄ±n yahudi yerleÅŸimcilere tepki gÃ¶stererek oturma eylemi dÃ¼zenledi israilli kolluk gÃ¼Ã§leri olayÄ± protesto eden filistinlilere gerÃ§ek mermilerle saldÄ±rdÄ±', 'filistinlilerden gerÃ§ek ise plastik mermilerle yaralanÄ±rken onlanca kiÅŸi ise atÄ±lan gÃ¶z yaÅŸartÄ±cÄ± gazdan etkilendi', 'israil polisinin dÃ¼n sabah saatlerinden itibaren aksanÄ±n kapÄ±larÄ±nda gÃ¼venlik Ã¶nlemlerini arttÄ±rdÄ±ÄŸÄ± ifade edildi', 'Ã¶te yandan dÃ¼n israil gazzede hamasm silahlÄ± kanadÄ± izzeddin elkassam tugayiarÄ±nÄ±n eÄŸitim alanÄ±na hava saldÄ±rÄ±sÄ± dÃ¼zenledi', 'saldÄ±rÄ±da Ã¶len ya da yaralanan olmadÄ±', 'imi lifi mm buj']\n",
      "--------------------\n",
      "haber filistinde yahudi yerleÅŸimcilerin polis korumasÄ±nda mescidi aksanÄ±n avlusuna girmesi gerginliÄŸe neden oldu yahudi yerleÅŸimcilerden oluÅŸan kiÅŸilik grup israil polisinin korumasÄ± altÄ±nda mescidi aksanÄ±n avlusuna girdi bunun Ã¼zerine birgrup filistinli gÃ¶nÃ¼llÃ¼ kadÄ±n yahudi yerleÅŸimcilere tepki gÃ¶stererek oturma eylemi dÃ¼zenledi israilli kolluk gÃ¼Ã§leri olayÄ± protesto eden filistinlilere gerÃ§ek mermilerle saldÄ±rdÄ± filistinlilerden gerÃ§ek ise plastik mermilerle yaralanÄ±rken onlanca kiÅŸi ise atÄ±lan gÃ¶z yaÅŸartÄ±cÄ± gazdan etkilendi israil polisinin dÃ¼n sabah saatlerinden itibaren aksanÄ±n kapÄ±larÄ±nda gÃ¼venlik Ã¶nlemlerini arttÄ±rdÄ±ÄŸÄ± ifade edildi Ã¶te yandan dÃ¼n israil gazzede hamasm silahlÄ± kanadÄ± izzeddin elkassam tugayiarÄ±nÄ±n eÄŸitim alanÄ±na hava saldÄ±rÄ±sÄ± dÃ¼zenledi saldÄ±rÄ±da Ã¶len ya da yaralanan olmadÄ± imi lifi mm buj\n",
      "hate\n"
     ]
    }
   ],
   "source": [
    "row = data.loc[19]\n",
    "print(row.title)\n",
    "print(row.sentences)\n",
    "print(\"--------------------\")\n",
    "print(row.text)\n",
    "print(row.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e197d25-219b-40d1-b427-936e854deb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data, open(\"../data/data_id-text-label_2022-10-14.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be644a2-1253-4f6a-96ca-6d099b1ca755",
   "metadata": {},
   "source": [
    "## Huggingface custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adae4e0f-e632-477e-ade0-6e92f65ea47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sahte polislerin kuryesi yakalandÄ± sahte polis...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>kÃ¼rt Ã¼z ama hain deÄŸiliz kÃ¼rtÃ¼z ama hain deÄŸil...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>suriyeli gelinden altÄ±n vurgunu kuyumcuda altÄ±...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>mustafa nevruz sÄ±nacÄ± mustafa nevruz sÄ±nacÄ± lg...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mustafa nevruz sÄ±nacÄ± mustafa nevruz sÄ±nacÄ± ya...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25061</th>\n",
       "      <td>25061</td>\n",
       "      <td>amnesty Ä±nternational ve global ahlaksÄ±zlÄ±k do...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25062</th>\n",
       "      <td>25062</td>\n",
       "      <td>Ã§anakkale asla unutulmamalÄ± llnutturulmamalÄ± Ã§...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25063</th>\n",
       "      <td>25063</td>\n",
       "      <td>sÃ¶mÃ¼rÃ¼ projesi olarak bop btp genel baÅŸkanÄ± pr...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25064</th>\n",
       "      <td>25064</td>\n",
       "      <td>doÄŸruluÅŸ zeminimiz helali bir millet istiklali...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25065</th>\n",
       "      <td>25065</td>\n",
       "      <td>yahudilikten islama yÃ¶nelen bir sahabi abdulla...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25066 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text Label\n",
       "0          0  sahte polislerin kuryesi yakalandÄ± sahte polis...  hate\n",
       "1          1  kÃ¼rt Ã¼z ama hain deÄŸiliz kÃ¼rtÃ¼z ama hain deÄŸil...  hate\n",
       "2          2  suriyeli gelinden altÄ±n vurgunu kuyumcuda altÄ±...  hate\n",
       "3          3  mustafa nevruz sÄ±nacÄ± mustafa nevruz sÄ±nacÄ± lg...  hate\n",
       "4          4  mustafa nevruz sÄ±nacÄ± mustafa nevruz sÄ±nacÄ± ya...  hate\n",
       "...      ...                                                ...   ...\n",
       "25061  25061  amnesty Ä±nternational ve global ahlaksÄ±zlÄ±k do...  hate\n",
       "25062  25062  Ã§anakkale asla unutulmamalÄ± llnutturulmamalÄ± Ã§...  hate\n",
       "25063  25063  sÃ¶mÃ¼rÃ¼ projesi olarak bop btp genel baÅŸkanÄ± pr...  hate\n",
       "25064  25064  doÄŸruluÅŸ zeminimiz helali bir millet istiklali...  hate\n",
       "25065  25065  yahudilikten islama yÃ¶nelen bir sahabi abdulla...  hate\n",
       "\n",
       "[25066 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pickle.load(open(\"../data/data_id-text-label_2022-10-14.pkl\", \"rb\"))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e2efb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>pub_name</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>Label</th>\n",
       "      <th>sentences</th>\n",
       "      <th>text</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>05 Ekim 2015 Pazartesi</td>\n",
       "      <td>akÅŸam</td>\n",
       "      <td>ulusal</td>\n",
       "      <td>sahte polislerin kuryesi yakalandÄ±</td>\n",
       "      <td>sahte polislerin kuryesi yakalandÄ± antalya'da ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[sahte polislerin kuryesi yakalandÄ± antalyada ...</td>\n",
       "      <td>sahte polislerin kuryesi yakalandÄ± sahte polis...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11 EylÃ¼l 2015 Cuma</td>\n",
       "      <td>akÅŸam</td>\n",
       "      <td>ulusal</td>\n",
       "      <td>kÃ¼rt Ã¼z ama hain deÄŸiliz</td>\n",
       "      <td>kÃ¼rt'Ã¼z ama hain deÄŸiliz suriye sÄ±nÄ±rÄ±nda devr...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[kÃ¼rtÃ¼z ama hain deÄŸiliz suriye sÄ±nÄ±rÄ±nda devr...</td>\n",
       "      <td>kÃ¼rt Ã¼z ama hain deÄŸiliz kÃ¼rtÃ¼z ama hain deÄŸil...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>25 EylÃ¼l 2015 Cuma</td>\n",
       "      <td>akÅŸam</td>\n",
       "      <td>ulusal</td>\n",
       "      <td>suriyeli gelinden altÄ±n vurgunu</td>\n",
       "      <td>kuyumcuda altÄ±n alÄ±rken fotoÄŸraf Ã§ektirdi. sur...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[kuyumcuda altÄ±n alÄ±rken fotoÄŸraf Ã§ektirdi, su...</td>\n",
       "      <td>suriyeli gelinden altÄ±n vurgunu kuyumcuda altÄ±...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>07 EylÃ¼l 2015 Pazartesi</td>\n",
       "      <td>anayurt</td>\n",
       "      <td>ulusal</td>\n",
       "      <td>mustafa nevruz sÄ±nacÄ±</td>\n",
       "      <td>mustafa nevruz sÄ±nacÄ± lgercek. abd'li yahudi, ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[mustafa nevruz sÄ±nacÄ± lgercek, abdli yahudi b...</td>\n",
       "      <td>mustafa nevruz sÄ±nacÄ± mustafa nevruz sÄ±nacÄ± lg...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>21 EylÃ¼l 2015 Pazartesi</td>\n",
       "      <td>anayurt</td>\n",
       "      <td>ulusal</td>\n",
       "      <td>mustafa nevruz sÄ±nacÄ±</td>\n",
       "      <td>mustafa nevruz sÄ±nacÄ± yazÄ±yor, gercek. abd'li ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[mustafa nevruz sÄ±nacÄ± yazÄ±yor gercek, abdli y...</td>\n",
       "      <td>mustafa nevruz sÄ±nacÄ± mustafa nevruz sÄ±nacÄ± ya...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25010</th>\n",
       "      <td>25061</td>\n",
       "      <td>02 MayÄ±s 2014 Cuma</td>\n",
       "      <td>yeni asya</td>\n",
       "      <td>hepsi</td>\n",
       "      <td>amnesty Ä±nternational ve global ahlaksÄ±zlÄ±k</td>\n",
       "      <td>doÄŸu veya batÄ± s. bulut@saidnursi. de amnesty ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[doÄŸu veya batÄ±, de amnesty Ä±nternational ve g...</td>\n",
       "      <td>amnesty Ä±nternational ve global ahlaksÄ±zlÄ±k do...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25011</th>\n",
       "      <td>25062</td>\n",
       "      <td>26 Mart 2014 Ã‡arÅŸamba</td>\n",
       "      <td>yeni konya</td>\n",
       "      <td>hepsi</td>\n",
       "      <td>Ã§anakkale asla unutulmamalÄ± llnutturulmamalÄ±</td>\n",
       "      <td>Ã§anakkale, asla unutulmamalÄ±, llnutturulmamalÄ±...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[Ã§anakkale asla unutulmamalÄ± llnutturulmamalÄ± ...</td>\n",
       "      <td>Ã§anakkale asla unutulmamalÄ± llnutturulmamalÄ± Ã§...</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25012</th>\n",
       "      <td>25063</td>\n",
       "      <td>04 Nisan 2014 Cuma</td>\n",
       "      <td>yeni mesaj</td>\n",
       "      <td>hepsi</td>\n",
       "      <td>sÃ¶mÃ¼rÃ¼ projesi olarak bop</td>\n",
       "      <td>btp genel baÅŸkanÄ± prof. dr. haydar bas Ä±n kale...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[btp genel baÅŸkanÄ± prof, haydar bas Ä±n kalemin...</td>\n",
       "      <td>sÃ¶mÃ¼rÃ¼ projesi olarak bop btp genel baÅŸkanÄ± pr...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25013</th>\n",
       "      <td>25064</td>\n",
       "      <td>24 Åubat 2014 Pazartesi</td>\n",
       "      <td>yeni mesaj</td>\n",
       "      <td>hepsi</td>\n",
       "      <td>doÄŸruluÅŸ zeminimiz helali bir millet istiklali...</td>\n",
       "      <td>prof. dr. nurullah Ã§etin doÄŸruluÅŸ zeminimiz: '...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[nurullah Ã§etin doÄŸruluÅŸ zeminimiz helali bir ...</td>\n",
       "      <td>doÄŸruluÅŸ zeminimiz helali bir millet istiklali...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25014</th>\n",
       "      <td>25065</td>\n",
       "      <td>17 Ocak 2014 Cuma</td>\n",
       "      <td>Ã§orum hakimiyet</td>\n",
       "      <td>hepsi</td>\n",
       "      <td>yahudilikten islama yÃ¶nelen bir sahabi abdulla...</td>\n",
       "      <td>yahudilikten islam'a yÃ¶nelen bir sahabi abdull...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[yahudilikten islama yÃ¶nelen bir sahabi abdull...</td>\n",
       "      <td>yahudilikten islama yÃ¶nelen bir sahabi abdulla...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25015 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                     date         pub_name    type  \\\n",
       "0          0   05 Ekim 2015 Pazartesi            akÅŸam  ulusal   \n",
       "1          1       11 EylÃ¼l 2015 Cuma            akÅŸam  ulusal   \n",
       "2          2       25 EylÃ¼l 2015 Cuma            akÅŸam  ulusal   \n",
       "3          3  07 EylÃ¼l 2015 Pazartesi          anayurt  ulusal   \n",
       "4          4  21 EylÃ¼l 2015 Pazartesi          anayurt  ulusal   \n",
       "...      ...                      ...              ...     ...   \n",
       "25010  25061       02 MayÄ±s 2014 Cuma        yeni asya   hepsi   \n",
       "25011  25062    26 Mart 2014 Ã‡arÅŸamba       yeni konya   hepsi   \n",
       "25012  25063       04 Nisan 2014 Cuma       yeni mesaj   hepsi   \n",
       "25013  25064  24 Åubat 2014 Pazartesi       yeni mesaj   hepsi   \n",
       "25014  25065        17 Ocak 2014 Cuma  Ã§orum hakimiyet   hepsi   \n",
       "\n",
       "                                                   title  \\\n",
       "0                     sahte polislerin kuryesi yakalandÄ±   \n",
       "1                               kÃ¼rt Ã¼z ama hain deÄŸiliz   \n",
       "2                        suriyeli gelinden altÄ±n vurgunu   \n",
       "3                                  mustafa nevruz sÄ±nacÄ±   \n",
       "4                                  mustafa nevruz sÄ±nacÄ±   \n",
       "...                                                  ...   \n",
       "25010        amnesty Ä±nternational ve global ahlaksÄ±zlÄ±k   \n",
       "25011       Ã§anakkale asla unutulmamalÄ± llnutturulmamalÄ±   \n",
       "25012                          sÃ¶mÃ¼rÃ¼ projesi olarak bop   \n",
       "25013  doÄŸruluÅŸ zeminimiz helali bir millet istiklali...   \n",
       "25014  yahudilikten islama yÃ¶nelen bir sahabi abdulla...   \n",
       "\n",
       "                                                 content Label  \\\n",
       "0      sahte polislerin kuryesi yakalandÄ± antalya'da ...  hate   \n",
       "1      kÃ¼rt'Ã¼z ama hain deÄŸiliz suriye sÄ±nÄ±rÄ±nda devr...  hate   \n",
       "2      kuyumcuda altÄ±n alÄ±rken fotoÄŸraf Ã§ektirdi. sur...  hate   \n",
       "3      mustafa nevruz sÄ±nacÄ± lgercek. abd'li yahudi, ...  hate   \n",
       "4      mustafa nevruz sÄ±nacÄ± yazÄ±yor, gercek. abd'li ...  hate   \n",
       "...                                                  ...   ...   \n",
       "25010  doÄŸu veya batÄ± s. bulut@saidnursi. de amnesty ...  hate   \n",
       "25011  Ã§anakkale, asla unutulmamalÄ±, llnutturulmamalÄ±...  hate   \n",
       "25012  btp genel baÅŸkanÄ± prof. dr. haydar bas Ä±n kale...  hate   \n",
       "25013  prof. dr. nurullah Ã§etin doÄŸruluÅŸ zeminimiz: '...  hate   \n",
       "25014  yahudilikten islam'a yÃ¶nelen bir sahabi abdull...  hate   \n",
       "\n",
       "                                               sentences  \\\n",
       "0      [sahte polislerin kuryesi yakalandÄ± antalyada ...   \n",
       "1      [kÃ¼rtÃ¼z ama hain deÄŸiliz suriye sÄ±nÄ±rÄ±nda devr...   \n",
       "2      [kuyumcuda altÄ±n alÄ±rken fotoÄŸraf Ã§ektirdi, su...   \n",
       "3      [mustafa nevruz sÄ±nacÄ± lgercek, abdli yahudi b...   \n",
       "4      [mustafa nevruz sÄ±nacÄ± yazÄ±yor gercek, abdli y...   \n",
       "...                                                  ...   \n",
       "25010  [doÄŸu veya batÄ±, de amnesty Ä±nternational ve g...   \n",
       "25011  [Ã§anakkale asla unutulmamalÄ± llnutturulmamalÄ± ...   \n",
       "25012  [btp genel baÅŸkanÄ± prof, haydar bas Ä±n kalemin...   \n",
       "25013  [nurullah Ã§etin doÄŸruluÅŸ zeminimiz helali bir ...   \n",
       "25014  [yahudilikten islama yÃ¶nelen bir sahabi abdull...   \n",
       "\n",
       "                                                    text  phase  \n",
       "0      sahte polislerin kuryesi yakalandÄ± sahte polis...  train  \n",
       "1      kÃ¼rt Ã¼z ama hain deÄŸiliz kÃ¼rtÃ¼z ama hain deÄŸil...   test  \n",
       "2      suriyeli gelinden altÄ±n vurgunu kuyumcuda altÄ±...   test  \n",
       "3      mustafa nevruz sÄ±nacÄ± mustafa nevruz sÄ±nacÄ± lg...  train  \n",
       "4      mustafa nevruz sÄ±nacÄ± mustafa nevruz sÄ±nacÄ± ya...  train  \n",
       "...                                                  ...    ...  \n",
       "25010  amnesty Ä±nternational ve global ahlaksÄ±zlÄ±k do...  train  \n",
       "25011  Ã§anakkale asla unutulmamalÄ± llnutturulmamalÄ± Ã§...    val  \n",
       "25012  sÃ¶mÃ¼rÃ¼ projesi olarak bop btp genel baÅŸkanÄ± pr...  train  \n",
       "25013  doÄŸruluÅŸ zeminimiz helali bir millet istiklali...  train  \n",
       "25014  yahudilikten islama yÃ¶nelen bir sahabi abdulla...  train  \n",
       "\n",
       "[25015 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8cf8e2f-c6ff-4d12-9164-ed5dcd5c3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04afaee0-c305-4900-b852-6c101eb53c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDVDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, idxs):\n",
    "        self.label_encodings = {\"not_hate\": 0, \"hate\": 1}\n",
    "        self.encodings = encodings\n",
    "        self.labels = [self.label_encodings[label] for label in labels]\n",
    "        self.idxs = idxs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4951616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, train_texts, train_labels = list(data.loc[data[\"phase\"] == \"train\", \"id\"].values), list(data.loc[data[\"phase\"] == \"train\", \"text\"].values), list(data.loc[data[\"phase\"] == \"train\", \"Label\"].values)\n",
    "val_idxs, val_texts, val_labels = list(data.loc[data[\"phase\"] == \"val\", \"id\"].values), list(data.loc[data[\"phase\"] == \"val\", \"text\"].values), list(data.loc[data[\"phase\"] == \"val\", \"Label\"].values)\n",
    "test_idxs, test_texts, test_labels = list(data.loc[data[\"phase\"] == \"test\", \"id\"].values), list(data.loc[data[\"phase\"] == \"test\", \"text\"].values), list(data.loc[data[\"phase\"] == \"test\", \"Label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9985b2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dist: Counter({'hate': 10074, 'not_hate': 9944})\n",
      "Validation dist: Counter({'hate': 1257, 'not_hate': 1243})\n",
      "Test dist: Counter({'hate': 1255, 'not_hate': 1242})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dist: {Counter(train_labels)}\")\n",
    "print(f\"Validation dist: {Counter(val_labels)}\")\n",
    "print(f\"Test dist: {Counter(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8afb469-387c-483a-8501-0212ec785468",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55d81c32-7f3e-40d9-8760-4e552000f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90f16f64-359d-4a7c-a092-141800b08a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HDVDataset(train_encodings, train_labels, train_idxs)\n",
    "val_dataset = HDVDataset(val_encodings, val_labels, val_idxs)\n",
    "test_dataset = HDVDataset(test_encodings, test_labels, test_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c599074-ae92-44fb-8af5-fe40c98255f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99600c4f-63ba-490e-a4bc-4824caeb784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 7.55kB [00:00, 1.62MB/s]                   \n",
      "Downloading builder script: 7.38kB [00:00, 1.53MB/s]                   \n",
      "Downloading builder script: 4.21kB [00:00, 1.07MB/s]                   \n",
      "Downloading builder script: 6.50kB [00:00, 5.02MB/s]                   \n"
     ]
    }
   ],
   "source": [
    "prec = load_metric(\"precision\")\n",
    "rec = load_metric(\"recall\")\n",
    "acc = load_metric(\"accuracy\")\n",
    "f1 = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    result = {}\n",
    "    for mtrc in [prec, rec, acc, f1]:\n",
    "        mtrc_result = mtrc.compute(predictions=predictions, references=labels)\n",
    "        result.update(mtrc_result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b0cc9-5464-4f6e-9350-9762310187cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Huggingface models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0d19972-4ecd-4b3d-9630-b69ea9a70f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=hdv_hate_speech\n",
      "env: WANDB_LOG_MODEL=true\n",
      "env: WANDB_WATCH=all\n",
      "env: WANDB_NOTEBOOK_NAME=berturk\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=hdv_hate_speech\n",
    "%env WANDB_LOG_MODEL=true\n",
    "%env WANDB_WATCH=all\n",
    "%env WANDB_NOTEBOOK_NAME=berturk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b920fcab-f99b-4000-8550-ed709ee3f11a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import wandb\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72bd936e-59fa-4392-bda8-1f9283888e45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find berturk.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnlpboun\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36d9fba1-cc80-4d54-9fcc-2a484dccfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"../experiments/results/\"\n",
    "logs_path = \"../experiments/logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc90eea2-815f-41d6-8952-ce0ad236515d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 706M/706M [03:47<00:00, 3.26MB/s] \n",
      "Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94fd1d23-3c14-4fcd-8668-28a3fc8b7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(results_path, \"berturk_128K_arute\"),               # output directory\n",
    "    num_train_epochs=2,                                                  # total number of training epochs\n",
    "    per_device_train_batch_size=4,                                       # batch size per device during training\n",
    "    per_device_eval_batch_size=4,                                        # batch size for evaluation\n",
    "    warmup_steps=500,                                                    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                                                   # strength of weight decay\n",
    "    logging_dir=os.path.join(results_path, \"berturk_128K_arute\"),              # directory for storing logs\n",
    "    logging_steps=20,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    save_steps=1000,\n",
    "    learning_rate=1e-05,\n",
    "    run_name=\"berturk_128K_uncased_lre-5\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                                                         # the instantiated ğŸ¤— Transformers model to be trained\n",
    "    args=training_args,                                                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,                                         # training dataset\n",
    "    eval_dataset=val_dataset,                                            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e80a4c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arute/miniconda3/envs/nlp_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20018\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1570' max='10010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1570/10010 03:59 < 21:28, 6.55 it/s, Epoch 0.31/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.575400</td>\n",
       "      <td>0.470825</td>\n",
       "      <td>0.778609</td>\n",
       "      <td>0.828162</td>\n",
       "      <td>0.795200</td>\n",
       "      <td>0.802621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.326600</td>\n",
       "      <td>0.417024</td>\n",
       "      <td>0.866092</td>\n",
       "      <td>0.879873</td>\n",
       "      <td>0.871200</td>\n",
       "      <td>0.872928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.452083</td>\n",
       "      <td>0.893755</td>\n",
       "      <td>0.876691</td>\n",
       "      <td>0.885600</td>\n",
       "      <td>0.885141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K_arute/checkpoint-1000\n",
      "Configuration saved in ../experiments/results/berturk_128K_arute/checkpoint-1000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K_arute/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/site-packages/transformers/trainer.py:1424\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1422\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 1424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1425\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1428\u001b[0m ):\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c410cbd2-5664-4938-a97e-bf879f92293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tabilab/anaconda3/envs/hate_env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 20052\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10026\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tabilab/projects/notebooks/wandb/run-20220414_135711-3vbi04su</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlpboun/hdv_hate_speech/runs/3vbi04su\" target=\"_blank\">berturk_128K_uncased_lre-5</a></strong> to <a href=\"https://wandb.ai/nlpboun/hdv_hate_speech\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10026' max='10026' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10026/10026 29:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.470382</td>\n",
       "      <td>0.742204</td>\n",
       "      <td>0.847310</td>\n",
       "      <td>0.774631</td>\n",
       "      <td>0.791282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.412100</td>\n",
       "      <td>0.487355</td>\n",
       "      <td>0.869938</td>\n",
       "      <td>0.883703</td>\n",
       "      <td>0.874751</td>\n",
       "      <td>0.876766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.880620</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>0.887515</td>\n",
       "      <td>0.889585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.537385</td>\n",
       "      <td>0.930530</td>\n",
       "      <td>0.805380</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>0.863444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.674700</td>\n",
       "      <td>0.489855</td>\n",
       "      <td>0.929856</td>\n",
       "      <td>0.818038</td>\n",
       "      <td>0.877144</td>\n",
       "      <td>0.870370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.378214</td>\n",
       "      <td>0.902478</td>\n",
       "      <td>0.893196</td>\n",
       "      <td>0.897487</td>\n",
       "      <td>0.897813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.367395</td>\n",
       "      <td>0.888298</td>\n",
       "      <td>0.924842</td>\n",
       "      <td>0.903470</td>\n",
       "      <td>0.906202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.372100</td>\n",
       "      <td>0.365355</td>\n",
       "      <td>0.894453</td>\n",
       "      <td>0.918513</td>\n",
       "      <td>0.904268</td>\n",
       "      <td>0.906323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.552500</td>\n",
       "      <td>0.460086</td>\n",
       "      <td>0.850035</td>\n",
       "      <td>0.959652</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>0.901524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.378452</td>\n",
       "      <td>0.916599</td>\n",
       "      <td>0.895570</td>\n",
       "      <td>0.906262</td>\n",
       "      <td>0.905962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.355999</td>\n",
       "      <td>0.912837</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.911448</td>\n",
       "      <td>0.912114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.388397</td>\n",
       "      <td>0.903502</td>\n",
       "      <td>0.918513</td>\n",
       "      <td>0.909454</td>\n",
       "      <td>0.910945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.430900</td>\n",
       "      <td>0.378103</td>\n",
       "      <td>0.898006</td>\n",
       "      <td>0.926424</td>\n",
       "      <td>0.909852</td>\n",
       "      <td>0.911994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.479300</td>\n",
       "      <td>0.377564</td>\n",
       "      <td>0.918285</td>\n",
       "      <td>0.897943</td>\n",
       "      <td>0.908257</td>\n",
       "      <td>0.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.251800</td>\n",
       "      <td>0.366917</td>\n",
       "      <td>0.887976</td>\n",
       "      <td>0.940665</td>\n",
       "      <td>0.910251</td>\n",
       "      <td>0.913561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.256600</td>\n",
       "      <td>0.377305</td>\n",
       "      <td>0.883309</td>\n",
       "      <td>0.946203</td>\n",
       "      <td>0.909852</td>\n",
       "      <td>0.913675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.361631</td>\n",
       "      <td>0.908949</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.915038</td>\n",
       "      <td>0.916438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.391033</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.925633</td>\n",
       "      <td>0.912246</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.371968</td>\n",
       "      <td>0.913590</td>\n",
       "      <td>0.920095</td>\n",
       "      <td>0.915836</td>\n",
       "      <td>0.916831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.367300</td>\n",
       "      <td>0.364992</td>\n",
       "      <td>0.901602</td>\n",
       "      <td>0.935127</td>\n",
       "      <td>0.915836</td>\n",
       "      <td>0.918058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-1000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-1000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-2000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-2000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-3000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-3000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-4000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-4000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-4000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-5000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-5000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-5000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-6000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-6000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-6000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-7000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-7000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-7000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-8000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-8000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-8000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-9000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-9000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-9000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-10000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-10000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-10000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../experiments/results/berturk_128K/checkpoint-10000 (score: 0.3649916350841522).\n",
      "Saving model checkpoint to /tmp/tmpu3xrqjht\n",
      "Configuration saved in /tmp/tmpu3xrqjht/config.json\n",
      "Model weights saved in /tmp/tmpu3xrqjht/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10026, training_loss=0.38614428408388934, metrics={'train_runtime': 1770.0672, 'train_samples_per_second': 22.657, 'train_steps_per_second': 5.664, 'total_flos': 1.055180576415744e+16, 'train_loss': 0.38614428408388934, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad39b6-4966-4c95-aec6-669ed22de961",
   "metadata": {},
   "source": [
    "## Evaluation (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3110b46c-0a69-4358-9ddf-380376b94000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2642' max='627' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [627/627 35:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3649916350841522,\n",
       " 'eval_precision': 0.9016018306636155,\n",
       " 'eval_recall': 0.935126582278481,\n",
       " 'eval_accuracy': 0.9158356601515756,\n",
       " 'eval_f1': 0.9180582524271845,\n",
       " 'eval_runtime': 22.3554,\n",
       " 'eval_samples_per_second': 112.143,\n",
       " 'eval_steps_per_second': 28.047,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = trainer.evaluate(val_dataset)\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8703b-9124-4c17-b393-218a9faa0524",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8421fb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2497\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [0 1 1 ... 1 1 1]\n",
      " GT's: [1 1 1 ... 1 1 1]\n",
      "{'test_loss': 0.4906734824180603, 'test_precision': 0.8367931281317108, 'test_recall': 0.9314741035856574, 'test_accuracy': 0.8742490989187024, 'test_f1': 0.8815987933634993, 'test_runtime': 17.5851, 'test_samples_per_second': 141.996, 'test_steps_per_second': 35.542}\n"
     ]
    }
   ],
   "source": [
    "preds_dict = trainer.predict(test_dataset)\n",
    "predictions = preds_dict.predictions\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print(f\"Preds: {predictions}\\n GT's: {preds_dict.label_ids}\")\n",
    "print(preds_dict.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20e05409-016d-43bb-b5cc-ad44ec3cace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDVDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts_idxs, labels, tokenizer):\n",
    "        self.label_encodings = {\"not_hate\": 0, \"hate\": 1}\n",
    "        self.rev_label_encodings = {0: \"not_hate\", 1: \"hate\"}\n",
    "        \n",
    "        self.texts, self.idxs = list(np.array(texts_idxs)[:, 0]), list(np.array(texts_idxs)[:, 1])\n",
    "        self.encodings = tokenizer(self.texts, truncation=True, padding=True)\n",
    "        self.labels = [self.label_encodings[label] for label in labels]\n",
    "        self.preds = []\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _get_preds_with_idx(self):\n",
    "        df_preds = pd.DataFrame(data={\"idx\": self.idxs, \"prediction\": self.preds})\n",
    "        df_preds[\"prediction\"] = df_preds[\"prediction\"].map(self.rev_label_encodings)\n",
    "        return df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b33ce1fa-1894-4280-8ce7-960e0da33ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_idxs, labels = list(data[[\"text\", \"id\"]].values), list(data[\"Label\"].values)\n",
    "train_texts_idxs_2, val_texts_idxs_2, train_labels_2, val_labels_2 = train_test_split(texts_idxs, labels, stratify=labels, test_size=.2, shuffle=True, random_state=17)\n",
    "val_texts_idxs_2, test_texts_idxs_2, val_labels_2, test_labels_2 = train_test_split(val_texts_idxs_2, val_labels_2, stratify=val_labels_2, test_size=.5, shuffle=True, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f944246f-86a6-4010-966d-08e99938b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_idx = HDVDatasetTest(test_texts_idxs_2, test_labels_2, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa51f1-9524-47db-8249-b6d24dddccdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7aa375aa-f4f4-4478-9b29-f0f4e68c527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1 0 1 ... 1 0 1]\n",
      " GT's: [1 0 1 ... 1 0 1]\n",
      "{'test_loss': 0.42105528712272644, 'test_precision': 0.8891419893697798, 'test_recall': 0.9271575613618369, 'test_accuracy': 0.9050658157159952, 'test_f1': 0.9077519379844963, 'test_runtime': 23.0777, 'test_samples_per_second': 108.633, 'test_steps_per_second': 27.169}\n"
     ]
    }
   ],
   "source": [
    "preds_dict = trainer.predict(test_dataset)\n",
    "predictions = preds_dict.predictions\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print(f\"Preds: {predictions}\\n GT's: {preds_dict.label_ids}\")\n",
    "print(preds_dict.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "209c979d-16db-49e2-b5fa-97f4e345ca5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37011e44-0fdd-492e-9ad8-0ca38221b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1 0 1 ... 1 0 1]\n",
      " GT's: [1 0 1 ... 1 0 1]\n",
      "{'test_loss': 0.42105528712272644, 'test_precision': 0.8891419893697798, 'test_recall': 0.9271575613618369, 'test_accuracy': 0.9050658157159952, 'test_f1': 0.9077519379844963, 'test_runtime': 23.2055, 'test_samples_per_second': 108.035, 'test_steps_per_second': 27.019}\n"
     ]
    }
   ],
   "source": [
    "preds_dict_2 = trainer.predict(test_dataset_idx)\n",
    "predictions_2 = preds_dict_2.predictions\n",
    "predictions_2 = np.argmax(predictions_2, axis=1)\n",
    "print(f\"Preds: {predictions_2}\\n GT's: {preds_dict_2.label_ids}\")\n",
    "print(preds_dict_2.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2585239a-7278-4a05-ad6d-5c79efe0fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_idx.preds = predictions_2\n",
    "df_preds = test_dataset_idx._get_preds_with_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d82d23b7-f456-4230-b3cd-79d07986bcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3973</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16849</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9186</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3072</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10346</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>3672</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>18446</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>3657</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>19535</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>8945</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2507 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx prediction\n",
       "0      3973       hate\n",
       "1     16849   not_hate\n",
       "2      9186       hate\n",
       "3      3072       hate\n",
       "4     10346       hate\n",
       "...     ...        ...\n",
       "2502   3672       hate\n",
       "2503  18446   not_hate\n",
       "2504   3657       hate\n",
       "2505  19535   not_hate\n",
       "2506   8945       hate\n",
       "\n",
       "[2507 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab619e57-c533-480e-bc2a-c32d0f174665",
   "metadata": {},
   "source": [
    "## Report to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbd5a102-9b0d-45be-9335-3fa46f999bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_preds = pd.merge(data, df_preds, left_on=\"id\", right_on=\"idx\", how=\"right\").drop(\"idx\", axis=1)\n",
    "df_label_preds.to_excel(\"../outputs/labels_preds_berturk_2022-04-14.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce0d0553-4d65-455f-bd9a-41a2c41b013a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3973</td>\n",
       "      <td>haber seyfullah koyuncu freddy mercurynin aske...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16849</td>\n",
       "      <td>kilisede hz fatÄ±ma nÄ±n doÄŸumu kutlandÄ± kilised...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9186</td>\n",
       "      <td>yunanlÄ±larÄ±n verdiÄŸi zararlarÄ± anlatan resmi d...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3072</td>\n",
       "      <td>itÄ± hr it serdar Ã§alÄ±ÅŸkan yaptÄ±ÄŸÄ± aÃ§Ä±klamada ÅŸ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10346</td>\n",
       "      <td>mÃ¼ltecileri dÃ¶vÃ¼p geri gÃ¶nderdiler mÃ¼ltecileri...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>3672</td>\n",
       "      <td>saitiyor su rl yÄ±u ur bpfjmkmmiami rj fiil il ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>18446</td>\n",
       "      <td>bu iÅŸbirllfii tÃ¼rkiye ye Ã¶rnek oucak bu iÅŸbirl...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>3657</td>\n",
       "      <td>mersinde iÅŸlenen cinayetle ilgili suriyeli tut...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>19535</td>\n",
       "      <td>sipariÅŸle kurulan proje Ã¶rgÃ¼tlerdir sipariÅŸle ...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>8945</td>\n",
       "      <td>iÃ§imizi iÃ§imizi dÃ¶ktÃ¼ÄŸÃ¼mÃ¼z duvarlar handan yal...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2507 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text     Label  \\\n",
       "0      3973  haber seyfullah koyuncu freddy mercurynin aske...      hate   \n",
       "1     16849  kilisede hz fatÄ±ma nÄ±n doÄŸumu kutlandÄ± kilised...  not_hate   \n",
       "2      9186  yunanlÄ±larÄ±n verdiÄŸi zararlarÄ± anlatan resmi d...      hate   \n",
       "3      3072  itÄ± hr it serdar Ã§alÄ±ÅŸkan yaptÄ±ÄŸÄ± aÃ§Ä±klamada ÅŸ...      hate   \n",
       "4     10346  mÃ¼ltecileri dÃ¶vÃ¼p geri gÃ¶nderdiler mÃ¼ltecileri...      hate   \n",
       "...     ...                                                ...       ...   \n",
       "2502   3672  saitiyor su rl yÄ±u ur bpfjmkmmiami rj fiil il ...      hate   \n",
       "2503  18446  bu iÅŸbirllfii tÃ¼rkiye ye Ã¶rnek oucak bu iÅŸbirl...  not_hate   \n",
       "2504   3657  mersinde iÅŸlenen cinayetle ilgili suriyeli tut...      hate   \n",
       "2505  19535  sipariÅŸle kurulan proje Ã¶rgÃ¼tlerdir sipariÅŸle ...  not_hate   \n",
       "2506   8945  iÃ§imizi iÃ§imizi dÃ¶ktÃ¼ÄŸÃ¼mÃ¼z duvarlar handan yal...      hate   \n",
       "\n",
       "     prediction  \n",
       "0          hate  \n",
       "1      not_hate  \n",
       "2          hate  \n",
       "3          hate  \n",
       "4          hate  \n",
       "...         ...  \n",
       "2502       hate  \n",
       "2503   not_hate  \n",
       "2504       hate  \n",
       "2505   not_hate  \n",
       "2506       hate  \n",
       "\n",
       "[2507 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a81a4df-ebec-4a0c-b12e-dee52fdd9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"../data/data_cleaned_sentences_2020-04-10.csv\", sep='|', converters={'sentences': pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ec565bc-ab4f-4756-ab2e-4d78bc1bb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = data_raw.reset_index().rename(columns={\"index\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c3c1a31-0846-4d15-9eb9-e902eeae20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_and_preds = pd.merge(data_raw, df_preds, left_on=\"id\", right_on=\"idx\", how=\"right\").drop(\"idx\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d26d628d-4c80-4ce2-87d3-e531957336a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_and_preds.to_excel(\"../outputs/data_and_preds_2022-04-14.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9e89c12-ec6d-4715-af9b-f77f0db75658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9050658157159952"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_data_and_preds[\"Label\"] == df_data_and_preds[\"prediction\"]) / df_data_and_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c4ff115a-25c0-41cd-9a99-9eb2b5c187c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hate': 1264, 'not_hate': 1243})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828fbda-52e1-47aa-a037-495319d60750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea187edb-7a5d-4522-ada6-506674386429",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../outputs/data_and_preds_2022-04-14.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0036d72-5356-426b-b367-3a6202140069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9046856227472968"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(df[\"Label\"] == df[\"prediction\"]) - len(remove_id_from_test)) / (df.shape[0] - len(remove_id_from_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d46c4-323b-4151-b093-920fc9a7609f",
   "metadata": {},
   "source": [
    "## Duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291e06b8-5b3a-4d4c-be19-8c0a104972f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_excel = pd.read_excel(\"../outputs/data_and_preds_2022-04-14.xlsx\")[\"id\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16084c1-14aa-4787-b137-3fcffd686f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.array(test_texts_idxs_2)[:, 1]) == id_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5234732-8ef7-4d34-9041-564f3570a220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set.intersection(set(np.array(test_texts_idxs_2)[:, 1]), set(np.array(train_texts_idxs_2)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f10f8-ebdf-47a0-b5c3-2eef44886513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8eafbcf3-f78e-430a-abaf-bced5e0f70d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pair = data[data[[\"text\"]].duplicated(keep=\"first\")].sort_values(\"text\").id.values.tolist()\n",
    "last_pair = data[data[[\"text\"]].duplicated(keep=\"last\")].sort_values(\"text\").id.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7831696a-0bd6-477a-998a-43b6d53b193a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [[first_pair[i], last_pair[i]] for i in range(len(first_pair))]\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99e7e8fc-a3b0-4d8d-9078-33117af2fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, val_idxs, test_idxs = list(np.array(train_texts_idxs_2)[:, 1]), list(np.array(val_texts_idxs_2)[:, 1]), list(np.array(test_texts_idxs_2)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22b1a03d-ff28-4c96-ae68-eb46c7a7de37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_duplicate_rows(train_idxs, test_idxs, pairs):\n",
    "    remove_id_from_test = []\n",
    "    for pair in pairs:\n",
    "        if pair[0] in train_idxs and pair[1] in train_idxs:\n",
    "            print(pair, \" is all in train set!\")\n",
    "        elif pair[0] in train_idxs and pair[1] in test_idxs:\n",
    "            print(f\"{pair[0]} in train, {pair[1]} in test, please delete from test!!!\")\n",
    "            remove_id_from_test.append(pair[1])\n",
    "        elif pair[1] in train_idxs and pair[0] in test_idxs:\n",
    "            print(f\"{pair[1]} in train, {pair[0]} in test, please delete from test!!!\")\n",
    "            remove_id_from_test.append(pair[0])\n",
    "        elif pair[0] in test_idxs and pair[1] in test_idxs:\n",
    "            print(f\"{pair[0]} in test, {pair[1]} in test, please delete one of them from test!!!\")\n",
    "            remove_id_from_test.append(pair[1])\n",
    "    remove_test_indices = []\n",
    "    for remove_id in remove_id_from_test:\n",
    "        remove_test_indices.append(test_idxs.index(remove_id))\n",
    "    remove_test_indices\n",
    "    return remove_id_from_test, remove_test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e138719f-89c6-47e0-9b25-9d34c9c61493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_id_from_test = detect_duplicate_rows(train_idxs, test_idxs, pairs)\n",
    "remove_id_from_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4ab89b3-c3ad-412b-bedf-d5b497cb6df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_id_from_val = detect_duplicate_rows(train_idxs, val_idxs, pairs)\n",
    "remove_id_from_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5fd6ab-6007-43db-8423-10ac8af732fe",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa3e05c3-543c-439c-b544-ac099672bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"../experiments/results/berturk_128K/checkpoint-10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52c3c2d3-765e-4610-98df-7f4683585bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDVDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts_idxs, labels, tokenizer, remove_idxs):\n",
    "        self.label_encodings = {\"not_hate\": 0, \"hate\": 1}\n",
    "        self.rev_label_encodings = {0: \"not_hate\", 1: \"hate\"}\n",
    "        \n",
    "        self.texts, self.idxs = list(np.array(texts_idxs)[:, 0]), list(np.array(texts_idxs)[:, 1])\n",
    "        print(len(self.idxs))\n",
    "        self.texts = [text for i, text in enumerate(self.texts) if i not in remove_idxs]\n",
    "        self.idxs = [idx for i, idx in enumerate(self.idxs) if i not in remove_idxs]\n",
    "        self.encodings = tokenizer(self.texts, truncation=True, padding=True)\n",
    "        self.labels = [self.label_encodings[label] for i, label in enumerate(labels) if i not in remove_idxs]\n",
    "        self.preds = []\n",
    "        print(len(self.idxs))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _get_preds_with_idx(self):\n",
    "        df_preds = pd.DataFrame(data={\"idx\": self.idxs, \"prediction\": self.preds})\n",
    "        df_preds[\"prediction\"] = df_preds[\"prediction\"].map(self.rev_label_encodings)\n",
    "        return df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3df4d9c6-04e3-430f-9b02-a2e6c5e21349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2507\n",
      "2497\n"
     ]
    }
   ],
   "source": [
    "test_dataset_idx_cleaned = HDVDatasetTest(test_texts_idxs_2, test_labels_2, tokenizer, remove_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e4c2db2-0a4a-4865-9981-fc789fcb3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                                                         # the instantiated ğŸ¤— Transformers model to be trained\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "542b9347-3921-44e9-8cab-ed99cb243699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2497\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1 0 1 ... 1 0 1]\n",
      " GT's: [1 0 1 ... 1 0 1]\n",
      "{'test_loss': 0.4224632978439331, 'test_precision': 0.8884644766997708, 'test_recall': 0.9266932270916335, 'test_accuracy': 0.9046856227472968, 'test_f1': 0.9071762870514819, 'test_runtime': 17.3161, 'test_samples_per_second': 144.201, 'test_steps_per_second': 18.076}\n"
     ]
    }
   ],
   "source": [
    "preds_dict_3 = trainer.predict(test_dataset_idx_cleaned)\n",
    "predictions_3 = preds_dict_3.predictions\n",
    "predictions_3 = np.argmax(predictions_3, axis=1)\n",
    "print(f\"Preds: {predictions_3}\\n GT's: {preds_dict_3.label_ids}\")\n",
    "print(preds_dict_3.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf03f58-29f3-4fd2-9000-a3b62809d059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP Env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
