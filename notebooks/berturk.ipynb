{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "791f8cbb-c4e2-4a0b-b2b9-1e3f65bc5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2e9a0-1d86-4eab-bd90-469707ffc1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandarallel.initialize(progress_bar=False, nb_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1111f-49ac-4cd1-85ab-318455fc96d2",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215f85c-0ede-47b2-929f-e61be14809e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/data_cleaned_sentences_2020-04-10.csv\", sep='|', converters={'sentences': pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285dd78-b54b-473d-ab7f-81abdece7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"title\"] = data[\"title\"].parallel_apply(lambda title: title if isinstance(title, str) else \"\") \n",
    "data[\"text\"] = data.parallel_apply(lambda row: \" \".join([sent for sent in [row[\"title\"]] + row[\"sentences\"]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834807b7-7aad-4eac-b292-b6422dce6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index().rename(columns={\"index\": \"id\"})\n",
    "data = data[[\"id\", \"text\", \"Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46bccf6-4239-411e-a588-dca8961c3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = data.loc[19]\n",
    "print(row.title)\n",
    "print(row.sentences)\n",
    "print(\"--------------------\")\n",
    "print(row.text)\n",
    "print(row.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e197d25-219b-40d1-b427-936e854deb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data, open(\"../data/data_id-text-label_2022-10-14.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be644a2-1253-4f6a-96ca-6d099b1ca755",
   "metadata": {},
   "source": [
    "## Huggingface custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adae4e0f-e632-477e-ade0-6e92f65ea47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sahte polislerin kuryesi yakaland覺 sahte polis...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>k羹rt 羹z ama hain deiliz k羹rt羹z ama hain deil...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>suriyeli gelinden alt覺n vurgunu kuyumcuda alt覺...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>mustafa nevruz s覺nac覺 mustafa nevruz s覺nac覺 lg...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mustafa nevruz s覺nac覺 mustafa nevruz s覺nac覺 ya...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25061</th>\n",
       "      <td>25061</td>\n",
       "      <td>amnesty 覺nternational ve global ahlaks覺zl覺k do...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25062</th>\n",
       "      <td>25062</td>\n",
       "      <td>癟anakkale asla unutulmamal覺 llnutturulmamal覺 癟...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25063</th>\n",
       "      <td>25063</td>\n",
       "      <td>s繹m羹r羹 projesi olarak bop btp genel bakan覺 pr...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25064</th>\n",
       "      <td>25064</td>\n",
       "      <td>dorulu zeminimiz helali bir millet istiklali...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25065</th>\n",
       "      <td>25065</td>\n",
       "      <td>yahudilikten islama y繹nelen bir sahabi abdulla...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25066 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text Label\n",
       "0          0  sahte polislerin kuryesi yakaland覺 sahte polis...  hate\n",
       "1          1  k羹rt 羹z ama hain deiliz k羹rt羹z ama hain deil...  hate\n",
       "2          2  suriyeli gelinden alt覺n vurgunu kuyumcuda alt覺...  hate\n",
       "3          3  mustafa nevruz s覺nac覺 mustafa nevruz s覺nac覺 lg...  hate\n",
       "4          4  mustafa nevruz s覺nac覺 mustafa nevruz s覺nac覺 ya...  hate\n",
       "...      ...                                                ...   ...\n",
       "25061  25061  amnesty 覺nternational ve global ahlaks覺zl覺k do...  hate\n",
       "25062  25062  癟anakkale asla unutulmamal覺 llnutturulmamal覺 癟...  hate\n",
       "25063  25063  s繹m羹r羹 projesi olarak bop btp genel bakan覺 pr...  hate\n",
       "25064  25064  dorulu zeminimiz helali bir millet istiklali...  hate\n",
       "25065  25065  yahudilikten islama y繹nelen bir sahabi abdulla...  hate\n",
       "\n",
       "[25066 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pickle.load(open(\"../data/data_id-text-label_2022-10-14.pkl\", \"rb\"))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8cf8e2f-c6ff-4d12-9164-ed5dcd5c3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04afaee0-c305-4900-b852-6c101eb53c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDVDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, idxs):\n",
    "        self.label_encodings = {\"not_hate\": 0, \"hate\": 1}\n",
    "        self.encodings = encodings\n",
    "        self.labels = [self.label_encodings[label] for label in labels]\n",
    "        self.idxs = self.idxs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b7fea18-7ba2-4ae9-9d2b-f1e53683eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs, texts, labels = list(data[\"id\"].values), list(data[\"text\"].values), list(data[\"Label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7064ec9e-d0a7-40c3-b217-0d989ebcc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, stratify=labels, test_size=.2, shuffle=True, random_state=17)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, stratify=val_labels, test_size=.5, shuffle=True, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a27b581-5130-4146-98c8-5ff66c8dea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dist: Counter({'hate': 10107, 'not_hate': 9945})\n",
      "Validation dist: Counter({'hate': 1264, 'not_hate': 1243})\n",
      "Test dist: Counter({'hate': 1263, 'not_hate': 1244})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dist: {Counter(train_labels)}\")\n",
    "print(f\"Validation dist: {Counter(val_labels)}\")\n",
    "print(f\"Test dist: {Counter(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8afb469-387c-483a-8501-0212ec785468",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d81c32-7f3e-40d9-8760-4e552000f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f16f64-359d-4a7c-a092-141800b08a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HDVDataset(train_encodings, train_labels)\n",
    "val_dataset = HDVDataset(val_encodings, val_labels)\n",
    "test_dataset = HDVDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c599074-ae92-44fb-8af5-fe40c98255f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "99600c4f-63ba-490e-a4bc-4824caeb784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = load_metric(\"precision\")\n",
    "rec = load_metric(\"recall\")\n",
    "acc = load_metric(\"accuracy\")\n",
    "f1 = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    result = {}\n",
    "    for mtrc in [prec, rec, acc, f1]:\n",
    "        mtrc_result = mtrc.compute(predictions=predictions, references=labels)\n",
    "        result.update(mtrc_result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b0cc9-5464-4f6e-9350-9762310187cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Huggingface models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0d19972-4ecd-4b3d-9630-b69ea9a70f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=hdv_hate_speech\n",
      "env: WANDB_LOG_MODEL=true\n",
      "env: WANDB_WATCH=all\n",
      "env: WANDB_NOTEBOOK_NAME=berturk\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=hdv_hate_speech\n",
    "%env WANDB_LOG_MODEL=true\n",
    "%env WANDB_WATCH=all\n",
    "%env WANDB_NOTEBOOK_NAME=berturk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b920fcab-f99b-4000-8550-ed709ee3f11a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72bd936e-59fa-4392-bda8-1f9283888e45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find berturk.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnlpboun\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36d9fba1-cc80-4d54-9fcc-2a484dccfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"../experiments/results/\"\n",
    "logs_path = \"../experiments/logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc90eea2-815f-41d6-8952-ce0ad236515d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94fd1d23-3c14-4fcd-8668-28a3fc8b7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(results_path, \"berturk_128K\"),               # output directory\n",
    "    num_train_epochs=2,                                                  # total number of training epochs\n",
    "    per_device_train_batch_size=4,                                       # batch size per device during training\n",
    "    per_device_eval_batch_size=4,                                        # batch size for evaluation\n",
    "    warmup_steps=500,                                                    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                                                   # strength of weight decay\n",
    "    logging_dir=os.path.join(results_path, \"berturk_128K\"),              # directory for storing logs\n",
    "    logging_steps=20,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    save_steps=1000,\n",
    "    learning_rate=1e-05,\n",
    "    report_to='wandb',\n",
    "    run_name=\"berturk_128K_uncased_lre-5\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                                                         # the instantiated  Transformers model to be trained\n",
    "    args=training_args,                                                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,                                         # training dataset\n",
    "    eval_dataset=val_dataset,                                            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c410cbd2-5664-4938-a97e-bf879f92293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tabilab/anaconda3/envs/hate_env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 20052\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10026\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tabilab/projects/notebooks/wandb/run-20220414_135711-3vbi04su</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlpboun/hdv_hate_speech/runs/3vbi04su\" target=\"_blank\">berturk_128K_uncased_lre-5</a></strong> to <a href=\"https://wandb.ai/nlpboun/hdv_hate_speech\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10026' max='10026' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10026/10026 29:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.470382</td>\n",
       "      <td>0.742204</td>\n",
       "      <td>0.847310</td>\n",
       "      <td>0.774631</td>\n",
       "      <td>0.791282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.412100</td>\n",
       "      <td>0.487355</td>\n",
       "      <td>0.869938</td>\n",
       "      <td>0.883703</td>\n",
       "      <td>0.874751</td>\n",
       "      <td>0.876766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.880620</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>0.887515</td>\n",
       "      <td>0.889585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.537385</td>\n",
       "      <td>0.930530</td>\n",
       "      <td>0.805380</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>0.863444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.674700</td>\n",
       "      <td>0.489855</td>\n",
       "      <td>0.929856</td>\n",
       "      <td>0.818038</td>\n",
       "      <td>0.877144</td>\n",
       "      <td>0.870370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.378214</td>\n",
       "      <td>0.902478</td>\n",
       "      <td>0.893196</td>\n",
       "      <td>0.897487</td>\n",
       "      <td>0.897813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.367395</td>\n",
       "      <td>0.888298</td>\n",
       "      <td>0.924842</td>\n",
       "      <td>0.903470</td>\n",
       "      <td>0.906202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.372100</td>\n",
       "      <td>0.365355</td>\n",
       "      <td>0.894453</td>\n",
       "      <td>0.918513</td>\n",
       "      <td>0.904268</td>\n",
       "      <td>0.906323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.552500</td>\n",
       "      <td>0.460086</td>\n",
       "      <td>0.850035</td>\n",
       "      <td>0.959652</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>0.901524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.378452</td>\n",
       "      <td>0.916599</td>\n",
       "      <td>0.895570</td>\n",
       "      <td>0.906262</td>\n",
       "      <td>0.905962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.355999</td>\n",
       "      <td>0.912837</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.911448</td>\n",
       "      <td>0.912114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.388397</td>\n",
       "      <td>0.903502</td>\n",
       "      <td>0.918513</td>\n",
       "      <td>0.909454</td>\n",
       "      <td>0.910945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.430900</td>\n",
       "      <td>0.378103</td>\n",
       "      <td>0.898006</td>\n",
       "      <td>0.926424</td>\n",
       "      <td>0.909852</td>\n",
       "      <td>0.911994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.479300</td>\n",
       "      <td>0.377564</td>\n",
       "      <td>0.918285</td>\n",
       "      <td>0.897943</td>\n",
       "      <td>0.908257</td>\n",
       "      <td>0.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.251800</td>\n",
       "      <td>0.366917</td>\n",
       "      <td>0.887976</td>\n",
       "      <td>0.940665</td>\n",
       "      <td>0.910251</td>\n",
       "      <td>0.913561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.256600</td>\n",
       "      <td>0.377305</td>\n",
       "      <td>0.883309</td>\n",
       "      <td>0.946203</td>\n",
       "      <td>0.909852</td>\n",
       "      <td>0.913675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.361631</td>\n",
       "      <td>0.908949</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.915038</td>\n",
       "      <td>0.916438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.391033</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.925633</td>\n",
       "      <td>0.912246</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.371968</td>\n",
       "      <td>0.913590</td>\n",
       "      <td>0.920095</td>\n",
       "      <td>0.915836</td>\n",
       "      <td>0.916831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.367300</td>\n",
       "      <td>0.364992</td>\n",
       "      <td>0.901602</td>\n",
       "      <td>0.935127</td>\n",
       "      <td>0.915836</td>\n",
       "      <td>0.918058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-1000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-1000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-2000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-2000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-3000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-3000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-4000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-4000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-4000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-5000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-5000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-5000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-6000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-6000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-6000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-7000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-7000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-7000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-8000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-8000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-8000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-9000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-9000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-9000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-10000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-10000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-10000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../experiments/results/berturk_128K/checkpoint-10000 (score: 0.3649916350841522).\n",
      "Saving model checkpoint to /tmp/tmpu3xrqjht\n",
      "Configuration saved in /tmp/tmpu3xrqjht/config.json\n",
      "Model weights saved in /tmp/tmpu3xrqjht/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10026, training_loss=0.38614428408388934, metrics={'train_runtime': 1770.0672, 'train_samples_per_second': 22.657, 'train_steps_per_second': 5.664, 'total_flos': 1.055180576415744e+16, 'train_loss': 0.38614428408388934, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad39b6-4966-4c95-aec6-669ed22de961",
   "metadata": {},
   "source": [
    "## Evaluation (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3110b46c-0a69-4358-9ddf-380376b94000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2642' max='627' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [627/627 35:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3649916350841522,\n",
       " 'eval_precision': 0.9016018306636155,\n",
       " 'eval_recall': 0.935126582278481,\n",
       " 'eval_accuracy': 0.9158356601515756,\n",
       " 'eval_f1': 0.9180582524271845,\n",
       " 'eval_runtime': 22.3554,\n",
       " 'eval_samples_per_second': 112.143,\n",
       " 'eval_steps_per_second': 28.047,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = trainer.evaluate(val_dataset)\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8703b-9124-4c17-b393-218a9faa0524",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20e05409-016d-43bb-b5cc-ad44ec3cace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDVDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts_idxs, labels, tokenizer):\n",
    "        self.label_encodings = {\"not_hate\": 0, \"hate\": 1}\n",
    "        self.rev_label_encodings = {0: \"not_hate\", 1: \"hate\"}\n",
    "        \n",
    "        self.texts, self.idxs = list(np.array(texts_idxs)[:, 0]), list(np.array(texts_idxs)[:, 1])\n",
    "        self.encodings = tokenizer(self.texts, truncation=True, padding=True)\n",
    "        self.labels = [self.label_encodings[label] for label in labels]\n",
    "        self.preds = []\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _get_preds_with_idx(self):\n",
    "        df_preds = pd.DataFrame(data={\"idx\": self.idxs, \"prediction\": self.preds})\n",
    "        df_preds[\"prediction\"] = df_preds[\"prediction\"].map(self.rev_label_encodings)\n",
    "        return df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b33ce1fa-1894-4280-8ce7-960e0da33ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_idxs, labels = list(data[[\"text\", \"id\"]].values), list(data[\"Label\"].values)\n",
    "train_texts_idxs_2, val_texts_idxs_2, train_labels_2, val_labels_2 = train_test_split(texts_idxs, labels, stratify=labels, test_size=.2, shuffle=True, random_state=17)\n",
    "val_texts_idxs_2, test_texts_idxs_2, val_labels_2, test_labels_2 = train_test_split(val_texts_idxs_2, val_labels_2, stratify=val_labels_2, test_size=.5, shuffle=True, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f944246f-86a6-4010-966d-08e99938b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_idx = HDVDatasetTest(test_texts_idxs_2, test_labels_2, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa51f1-9524-47db-8249-b6d24dddccdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7aa375aa-f4f4-4478-9b29-f0f4e68c527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1 0 1 ... 1 0 1]\n",
      " GT's: [1 0 1 ... 1 0 1]\n",
      "{'test_loss': 0.42105528712272644, 'test_precision': 0.8891419893697798, 'test_recall': 0.9271575613618369, 'test_accuracy': 0.9050658157159952, 'test_f1': 0.9077519379844963, 'test_runtime': 23.0777, 'test_samples_per_second': 108.633, 'test_steps_per_second': 27.169}\n"
     ]
    }
   ],
   "source": [
    "preds_dict = trainer.predict(test_dataset)\n",
    "predictions = preds_dict.predictions\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print(f\"Preds: {predictions}\\n GT's: {preds_dict.label_ids}\")\n",
    "print(preds_dict.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "209c979d-16db-49e2-b5fa-97f4e345ca5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37011e44-0fdd-492e-9ad8-0ca38221b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1 0 1 ... 1 0 1]\n",
      " GT's: [1 0 1 ... 1 0 1]\n",
      "{'test_loss': 0.42105528712272644, 'test_precision': 0.8891419893697798, 'test_recall': 0.9271575613618369, 'test_accuracy': 0.9050658157159952, 'test_f1': 0.9077519379844963, 'test_runtime': 23.2055, 'test_samples_per_second': 108.035, 'test_steps_per_second': 27.019}\n"
     ]
    }
   ],
   "source": [
    "preds_dict_2 = trainer.predict(test_dataset_idx)\n",
    "predictions_2 = preds_dict_2.predictions\n",
    "predictions_2 = np.argmax(predictions_2, axis=1)\n",
    "print(f\"Preds: {predictions_2}\\n GT's: {preds_dict_2.label_ids}\")\n",
    "print(preds_dict_2.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2585239a-7278-4a05-ad6d-5c79efe0fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_idx.preds = predictions_2\n",
    "df_preds = test_dataset_idx._get_preds_with_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d82d23b7-f456-4230-b3cd-79d07986bcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3973</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16849</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9186</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3072</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10346</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>3672</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>18446</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>3657</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>19535</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>8945</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2507 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx prediction\n",
       "0      3973       hate\n",
       "1     16849   not_hate\n",
       "2      9186       hate\n",
       "3      3072       hate\n",
       "4     10346       hate\n",
       "...     ...        ...\n",
       "2502   3672       hate\n",
       "2503  18446   not_hate\n",
       "2504   3657       hate\n",
       "2505  19535   not_hate\n",
       "2506   8945       hate\n",
       "\n",
       "[2507 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab619e57-c533-480e-bc2a-c32d0f174665",
   "metadata": {},
   "source": [
    "## Report to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbd5a102-9b0d-45be-9335-3fa46f999bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_preds = pd.merge(data, df_preds, left_on=\"id\", right_on=\"idx\", how=\"right\").drop(\"idx\", axis=1)\n",
    "df_label_preds.to_excel(\"../outputs/labels_preds_berturk_2022-04-14.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce0d0553-4d65-455f-bd9a-41a2c41b013a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3973</td>\n",
       "      <td>haber seyfullah koyuncu freddy mercurynin aske...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16849</td>\n",
       "      <td>kilisede hz fat覺ma n覺n doumu kutland覺 kilised...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9186</td>\n",
       "      <td>yunanl覺lar覺n verdii zararlar覺 anlatan resmi d...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3072</td>\n",
       "      <td>it覺 hr it serdar 癟al覺kan yapt覺覺 a癟覺klamada ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10346</td>\n",
       "      <td>m羹ltecileri d繹v羹p geri g繹nderdiler m羹ltecileri...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>3672</td>\n",
       "      <td>saitiyor su rl y覺u ur bpfjmkmmiami rj fiil il ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>18446</td>\n",
       "      <td>bu ibirllfii t羹rkiye ye 繹rnek oucak bu ibirl...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>3657</td>\n",
       "      <td>mersinde ilenen cinayetle ilgili suriyeli tut...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>19535</td>\n",
       "      <td>siparile kurulan proje 繹rg羹tlerdir siparile ...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>8945</td>\n",
       "      <td>i癟imizi i癟imizi d繹kt羹羹m羹z duvarlar handan yal...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2507 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text     Label  \\\n",
       "0      3973  haber seyfullah koyuncu freddy mercurynin aske...      hate   \n",
       "1     16849  kilisede hz fat覺ma n覺n doumu kutland覺 kilised...  not_hate   \n",
       "2      9186  yunanl覺lar覺n verdii zararlar覺 anlatan resmi d...      hate   \n",
       "3      3072  it覺 hr it serdar 癟al覺kan yapt覺覺 a癟覺klamada ...      hate   \n",
       "4     10346  m羹ltecileri d繹v羹p geri g繹nderdiler m羹ltecileri...      hate   \n",
       "...     ...                                                ...       ...   \n",
       "2502   3672  saitiyor su rl y覺u ur bpfjmkmmiami rj fiil il ...      hate   \n",
       "2503  18446  bu ibirllfii t羹rkiye ye 繹rnek oucak bu ibirl...  not_hate   \n",
       "2504   3657  mersinde ilenen cinayetle ilgili suriyeli tut...      hate   \n",
       "2505  19535  siparile kurulan proje 繹rg羹tlerdir siparile ...  not_hate   \n",
       "2506   8945  i癟imizi i癟imizi d繹kt羹羹m羹z duvarlar handan yal...      hate   \n",
       "\n",
       "     prediction  \n",
       "0          hate  \n",
       "1      not_hate  \n",
       "2          hate  \n",
       "3          hate  \n",
       "4          hate  \n",
       "...         ...  \n",
       "2502       hate  \n",
       "2503   not_hate  \n",
       "2504       hate  \n",
       "2505   not_hate  \n",
       "2506       hate  \n",
       "\n",
       "[2507 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a81a4df-ebec-4a0c-b12e-dee52fdd9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"../data/data_cleaned_sentences_2020-04-10.csv\", sep='|', converters={'sentences': pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ec565bc-ab4f-4756-ab2e-4d78bc1bb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = data_raw.reset_index().rename(columns={\"index\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c3c1a31-0846-4d15-9eb9-e902eeae20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_and_preds = pd.merge(data_raw, df_preds, left_on=\"id\", right_on=\"idx\", how=\"right\").drop(\"idx\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d26d628d-4c80-4ce2-87d3-e531957336a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_and_preds.to_excel(\"../outputs/data_and_preds_2022-04-14.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9e89c12-ec6d-4715-af9b-f77f0db75658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9050658157159952"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_data_and_preds[\"Label\"] == df_data_and_preds[\"prediction\"]) / df_data_and_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c4ff115a-25c0-41cd-9a99-9eb2b5c187c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hate': 1264, 'not_hate': 1243})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828fbda-52e1-47aa-a037-495319d60750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea187edb-7a5d-4522-ada6-506674386429",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../outputs/data_and_preds_2022-04-14.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0036d72-5356-426b-b367-3a6202140069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9046856227472968"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(df[\"Label\"] == df[\"prediction\"]) - len(remove_id_from_test)) / (df.shape[0] - len(remove_id_from_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d46c4-323b-4151-b093-920fc9a7609f",
   "metadata": {},
   "source": [
    "## Duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291e06b8-5b3a-4d4c-be19-8c0a104972f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_excel = pd.read_excel(\"../outputs/data_and_preds_2022-04-14.xlsx\")[\"id\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16084c1-14aa-4787-b137-3fcffd686f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.array(test_texts_idxs_2)[:, 1]) == id_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5234732-8ef7-4d34-9041-564f3570a220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set.intersection(set(np.array(test_texts_idxs_2)[:, 1]), set(np.array(train_texts_idxs_2)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f10f8-ebdf-47a0-b5c3-2eef44886513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8eafbcf3-f78e-430a-abaf-bced5e0f70d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pair = data[data[[\"text\"]].duplicated(keep=\"first\")].sort_values(\"text\").id.values.tolist()\n",
    "last_pair = data[data[[\"text\"]].duplicated(keep=\"last\")].sort_values(\"text\").id.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7831696a-0bd6-477a-998a-43b6d53b193a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15058, 2247],\n",
       " [886, 855],\n",
       " [1658, 1653],\n",
       " [270, 269],\n",
       " [8027, 8026],\n",
       " [888, 887],\n",
       " [3733, 3732],\n",
       " [7759, 7758],\n",
       " [1968, 1967],\n",
       " [6104, 6095],\n",
       " [7982, 7861],\n",
       " [11343, 2715],\n",
       " [9919, 9705],\n",
       " [951, 933],\n",
       " [3124, 3117],\n",
       " [11376, 11368],\n",
       " [19910, 7489],\n",
       " [10388, 10130],\n",
       " [998, 997],\n",
       " [8034, 7955],\n",
       " [2530, 2529],\n",
       " [554, 545],\n",
       " [8926, 8925],\n",
       " [22303, 22298],\n",
       " [4438, 4432],\n",
       " [6020, 6015],\n",
       " [860, 828],\n",
       " [1243, 1141],\n",
       " [1164, 1163],\n",
       " [973, 972],\n",
       " [11053, 9867],\n",
       " [6744, 6400],\n",
       " [7632, 7631],\n",
       " [19117, 6778],\n",
       " [3607, 3590],\n",
       " [1054, 1011],\n",
       " [11955, 11945],\n",
       " [24835, 24800],\n",
       " [2110, 2109],\n",
       " [3218, 3217],\n",
       " [11342, 2714],\n",
       " [557, 539],\n",
       " [5744, 5742],\n",
       " [21625, 21619],\n",
       " [7201, 7179],\n",
       " [1804, 1788],\n",
       " [12017, 12002],\n",
       " [7984, 7862],\n",
       " [11374, 11367],\n",
       " [16850, 3637],\n",
       " [2449, 2448]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [[first_pair[i], last_pair[i]] for i in range(len(first_pair))]\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99e7e8fc-a3b0-4d8d-9078-33117af2fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, val_idxs, test_idxs = list(np.array(train_texts_idxs_2)[:, 1]), list(np.array(val_texts_idxs_2)[:, 1]), list(np.array(test_texts_idxs_2)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22b1a03d-ff28-4c96-ae68-eb46c7a7de37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_duplicate_rows(train_idxs, test_idxs, pairs):\n",
    "    remove_id_from_test = []\n",
    "    for pair in pairs:\n",
    "        if pair[0] in train_idxs and pair[1] in train_idxs:\n",
    "            print(pair, \" is all in train set!\")\n",
    "        elif pair[0] in train_idxs and pair[1] in test_idxs:\n",
    "            print(f\"{pair[0]} in train, {pair[1]} in test, please delete from test!!!\")\n",
    "            remove_id_from_test.append(pair[1])\n",
    "        elif pair[1] in train_idxs and pair[0] in test_idxs:\n",
    "            print(f\"{pair[1]} in train, {pair[0]} in test, please delete from test!!!\")\n",
    "            remove_id_from_test.append(pair[0])\n",
    "        elif pair[0] in test_idxs and pair[1] in test_idxs:\n",
    "            print(f\"{pair[0]} in test, {pair[1]} in test, please delete one of them from test!!!\")\n",
    "            remove_id_from_test.append(pair[1])\n",
    "    remove_test_indices = []\n",
    "    for remove_id in remove_id_from_test:\n",
    "        remove_test_indices.append(test_idxs.index(remove_id))\n",
    "    remove_test_indices\n",
    "    return remove_id_from_test, remove_test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e138719f-89c6-47e0-9b25-9d34c9c61493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15058, 2247]  is all in train set!\n",
      "[886, 855]  is all in train set!\n",
      "[1658, 1653]  is all in train set!\n",
      "[270, 269]  is all in train set!\n",
      "[8027, 8026]  is all in train set!\n",
      "888 in train, 887 in test, please delete from test!!!\n",
      "[7759, 7758]  is all in train set!\n",
      "6104 in train, 6095 in test, please delete from test!!!\n",
      "[11343, 2715]  is all in train set!\n",
      "[9919, 9705]  is all in train set!\n",
      "[951, 933]  is all in train set!\n",
      "[3124, 3117]  is all in train set!\n",
      "[11376, 11368]  is all in train set!\n",
      "[10388, 10130]  is all in train set!\n",
      "[998, 997]  is all in train set!\n",
      "[8034, 7955]  is all in train set!\n",
      "[2530, 2529]  is all in train set!\n",
      "545 in train, 554 in test, please delete from test!!!\n",
      "[8926, 8925]  is all in train set!\n",
      "22303 in train, 22298 in test, please delete from test!!!\n",
      "[4438, 4432]  is all in train set!\n",
      "[6020, 6015]  is all in train set!\n",
      "[860, 828]  is all in train set!\n",
      "[1243, 1141]  is all in train set!\n",
      "[1164, 1163]  is all in train set!\n",
      "[973, 972]  is all in train set!\n",
      "[11053, 9867]  is all in train set!\n",
      "[6744, 6400]  is all in train set!\n",
      "[7632, 7631]  is all in train set!\n",
      "[3607, 3590]  is all in train set!\n",
      "1054 in train, 1011 in test, please delete from test!!!\n",
      "[11955, 11945]  is all in train set!\n",
      "[24835, 24800]  is all in train set!\n",
      "[2110, 2109]  is all in train set!\n",
      "3218 in test, 3217 in test, please delete one of them from test!!!\n",
      "[11342, 2714]  is all in train set!\n",
      "21625 in test, 21619 in test, please delete one of them from test!!!\n",
      "7179 in train, 7201 in test, please delete from test!!!\n",
      "1804 in test, 1788 in test, please delete one of them from test!!!\n",
      "[12017, 12002]  is all in train set!\n",
      "[7984, 7862]  is all in train set!\n",
      "[11374, 11367]  is all in train set!\n",
      "16850 in test, 3637 in test, please delete one of them from test!!!\n",
      "[2449, 2448]  is all in train set!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[887, 6095, 554, 22298, 1011, 3217, 21619, 7201, 1788, 3637]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_id_from_test = detect_duplicate_rows(train_idxs, test_idxs, pairs)\n",
    "remove_id_from_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4ab89b3-c3ad-412b-bedf-d5b497cb6df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15058, 2247]  is all in train set!\n",
      "[886, 855]  is all in train set!\n",
      "[1658, 1653]  is all in train set!\n",
      "[270, 269]  is all in train set!\n",
      "[8027, 8026]  is all in train set!\n",
      "3732 in train, 3733 in test, please delete from test!!!\n",
      "[7759, 7758]  is all in train set!\n",
      "1967 in train, 1968 in test, please delete from test!!!\n",
      "[11343, 2715]  is all in train set!\n",
      "[9919, 9705]  is all in train set!\n",
      "[951, 933]  is all in train set!\n",
      "[3124, 3117]  is all in train set!\n",
      "[11376, 11368]  is all in train set!\n",
      "19910 in train, 7489 in test, please delete from test!!!\n",
      "[10388, 10130]  is all in train set!\n",
      "[998, 997]  is all in train set!\n",
      "[8034, 7955]  is all in train set!\n",
      "[2530, 2529]  is all in train set!\n",
      "[8926, 8925]  is all in train set!\n",
      "[4438, 4432]  is all in train set!\n",
      "[6020, 6015]  is all in train set!\n",
      "[860, 828]  is all in train set!\n",
      "[1243, 1141]  is all in train set!\n",
      "[1164, 1163]  is all in train set!\n",
      "[973, 972]  is all in train set!\n",
      "[11053, 9867]  is all in train set!\n",
      "[6744, 6400]  is all in train set!\n",
      "[7632, 7631]  is all in train set!\n",
      "19117 in train, 6778 in test, please delete from test!!!\n",
      "[3607, 3590]  is all in train set!\n",
      "[11955, 11945]  is all in train set!\n",
      "[24835, 24800]  is all in train set!\n",
      "[2110, 2109]  is all in train set!\n",
      "[11342, 2714]  is all in train set!\n",
      "557 in train, 539 in test, please delete from test!!!\n",
      "5744 in train, 5742 in test, please delete from test!!!\n",
      "[12017, 12002]  is all in train set!\n",
      "[7984, 7862]  is all in train set!\n",
      "[11374, 11367]  is all in train set!\n",
      "[2449, 2448]  is all in train set!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3733, 1968, 7489, 6778, 539, 5742]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_id_from_val = detect_duplicate_rows(train_idxs, val_idxs, pairs)\n",
    "remove_id_from_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5fd6ab-6007-43db-8423-10ac8af732fe",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa3e05c3-543c-439c-b544-ac099672bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"../experiments/results/berturk_128K/checkpoint-10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52c3c2d3-765e-4610-98df-7f4683585bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDVDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts_idxs, labels, tokenizer, remove_idxs):\n",
    "        self.label_encodings = {\"not_hate\": 0, \"hate\": 1}\n",
    "        self.rev_label_encodings = {0: \"not_hate\", 1: \"hate\"}\n",
    "        \n",
    "        self.texts, self.idxs = list(np.array(texts_idxs)[:, 0]), list(np.array(texts_idxs)[:, 1])\n",
    "        print(len(self.idxs))\n",
    "        self.texts = [text for i, text in enumerate(self.texts) if i not in remove_idxs]\n",
    "        self.idxs = [idx for i, idx in enumerate(self.idxs) if i not in remove_idxs]\n",
    "        self.encodings = tokenizer(self.texts, truncation=True, padding=True)\n",
    "        self.labels = [self.label_encodings[label] for i, label in enumerate(labels) if i not in remove_idxs]\n",
    "        self.preds = []\n",
    "        print(len(self.idxs))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _get_preds_with_idx(self):\n",
    "        df_preds = pd.DataFrame(data={\"idx\": self.idxs, \"prediction\": self.preds})\n",
    "        df_preds[\"prediction\"] = df_preds[\"prediction\"].map(self.rev_label_encodings)\n",
    "        return df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3df4d9c6-04e3-430f-9b02-a2e6c5e21349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2507\n",
      "2497\n"
     ]
    }
   ],
   "source": [
    "test_dataset_idx_cleaned = HDVDatasetTest(test_texts_idxs_2, test_labels_2, tokenizer, remove_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e4c2db2-0a4a-4865-9981-fc789fcb3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                                                         # the instantiated  Transformers model to be trained\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "542b9347-3921-44e9-8cab-ed99cb243699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2497\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1 0 1 ... 1 0 1]\n",
      " GT's: [1 0 1 ... 1 0 1]\n",
      "{'test_loss': 0.4224632978439331, 'test_precision': 0.8884644766997708, 'test_recall': 0.9266932270916335, 'test_accuracy': 0.9046856227472968, 'test_f1': 0.9071762870514819, 'test_runtime': 17.3161, 'test_samples_per_second': 144.201, 'test_steps_per_second': 18.076}\n"
     ]
    }
   ],
   "source": [
    "preds_dict_3 = trainer.predict(test_dataset_idx_cleaned)\n",
    "predictions_3 = preds_dict_3.predictions\n",
    "predictions_3 = np.argmax(predictions_3, axis=1)\n",
    "print(f\"Preds: {predictions_3}\\n GT's: {preds_dict_3.label_ids}\")\n",
    "print(preds_dict_3.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf03f58-29f3-4fd2-9000-a3b62809d059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hate env",
   "language": "python",
   "name": "hate_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
