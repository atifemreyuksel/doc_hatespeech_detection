{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "791f8cbb-c4e2-4a0b-b2b9-1e3f65bc5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db2e9a0-1d86-4eab-bd90-469707ffc1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar=False, nb_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1111f-49ac-4cd1-85ab-318455fc96d2",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b215f85c-0ede-47b2-929f-e61be14809e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../data/data_cleaned_sentences_phases_2020-04-16.csv\", sep='|', converters={'sentences': pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e285dd78-b54b-473d-ab7f-81abdece7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"title\"] = data[\"title\"].parallel_apply(lambda title: title if isinstance(title, str) else \"\") \n",
    "data[\"text\"] = data.parallel_apply(lambda row: \" \".join([sent for sent in [row[\"title\"]] + row[\"sentences\"]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834807b7-7aad-4eac-b292-b6422dce6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index().rename(columns={\"index\": \"id\"})\n",
    "data = data[[\"id\", \"text\", \"Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46bccf6-4239-411e-a588-dca8961c3390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haber\n",
      "['filistinde yahudi yerleşimcilerin polis korumasında mescidi aksanın avlusuna girmesi gerginliğe neden oldu', 'yahudi yerleşimcilerden oluşan kişilik grup israil polisinin koruması altında mescidi aksanın avlusuna girdi', 'bunun üzerine birgrup filistinli gönüllü kadın yahudi yerleşimcilere tepki göstererek oturma eylemi düzenledi israilli kolluk güçleri olayı protesto eden filistinlilere gerçek mermilerle saldırdı', 'filistinlilerden gerçek ise plastik mermilerle yaralanırken onlanca kişi ise atılan göz yaşartıcı gazdan etkilendi', 'israil polisinin dün sabah saatlerinden itibaren aksanın kapılarında güvenlik önlemlerini arttırdığı ifade edildi', 'öte yandan dün israil gazzede hamasm silahlı kanadı izzeddin elkassam tugayiarının eğitim alanına hava saldırısı düzenledi', 'saldırıda ölen ya da yaralanan olmadı', 'imi lifi mm buj']\n",
      "--------------------\n",
      "haber filistinde yahudi yerleşimcilerin polis korumasında mescidi aksanın avlusuna girmesi gerginliğe neden oldu yahudi yerleşimcilerden oluşan kişilik grup israil polisinin koruması altında mescidi aksanın avlusuna girdi bunun üzerine birgrup filistinli gönüllü kadın yahudi yerleşimcilere tepki göstererek oturma eylemi düzenledi israilli kolluk güçleri olayı protesto eden filistinlilere gerçek mermilerle saldırdı filistinlilerden gerçek ise plastik mermilerle yaralanırken onlanca kişi ise atılan göz yaşartıcı gazdan etkilendi israil polisinin dün sabah saatlerinden itibaren aksanın kapılarında güvenlik önlemlerini arttırdığı ifade edildi öte yandan dün israil gazzede hamasm silahlı kanadı izzeddin elkassam tugayiarının eğitim alanına hava saldırısı düzenledi saldırıda ölen ya da yaralanan olmadı imi lifi mm buj\n",
      "hate\n"
     ]
    }
   ],
   "source": [
    "row = data.loc[19]\n",
    "print(row.title)\n",
    "print(row.sentences)\n",
    "print(\"--------------------\")\n",
    "print(row.text)\n",
    "print(row.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e197d25-219b-40d1-b427-936e854deb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data, open(\"../data/data_id-text-label_2022-10-14.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be644a2-1253-4f6a-96ca-6d099b1ca755",
   "metadata": {},
   "source": [
    "## Huggingface custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adae4e0f-e632-477e-ade0-6e92f65ea47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sahte polislerin kuryesi yakalandı sahte polis...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>kürt üz ama hain değiliz kürtüz ama hain değil...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>suriyeli gelinden altın vurgunu kuyumcuda altı...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>mustafa nevruz sınacı mustafa nevruz sınacı lg...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mustafa nevruz sınacı mustafa nevruz sınacı ya...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25061</th>\n",
       "      <td>25061</td>\n",
       "      <td>amnesty ınternational ve global ahlaksızlık do...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25062</th>\n",
       "      <td>25062</td>\n",
       "      <td>çanakkale asla unutulmamalı llnutturulmamalı ç...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25063</th>\n",
       "      <td>25063</td>\n",
       "      <td>sömürü projesi olarak bop btp genel başkanı pr...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25064</th>\n",
       "      <td>25064</td>\n",
       "      <td>doğruluş zeminimiz helali bir millet istiklali...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25065</th>\n",
       "      <td>25065</td>\n",
       "      <td>yahudilikten islama yönelen bir sahabi abdulla...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25066 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text Label\n",
       "0          0  sahte polislerin kuryesi yakalandı sahte polis...  hate\n",
       "1          1  kürt üz ama hain değiliz kürtüz ama hain değil...  hate\n",
       "2          2  suriyeli gelinden altın vurgunu kuyumcuda altı...  hate\n",
       "3          3  mustafa nevruz sınacı mustafa nevruz sınacı lg...  hate\n",
       "4          4  mustafa nevruz sınacı mustafa nevruz sınacı ya...  hate\n",
       "...      ...                                                ...   ...\n",
       "25061  25061  amnesty ınternational ve global ahlaksızlık do...  hate\n",
       "25062  25062  çanakkale asla unutulmamalı llnutturulmamalı ç...  hate\n",
       "25063  25063  sömürü projesi olarak bop btp genel başkanı pr...  hate\n",
       "25064  25064  doğruluş zeminimiz helali bir millet istiklali...  hate\n",
       "25065  25065  yahudilikten islama yönelen bir sahabi abdulla...  hate\n",
       "\n",
       "[25066 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pickle.load(open(\"../data/data_id-text-label_2022-10-14.pkl\", \"rb\"))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e2efb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>pub_name</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>Label</th>\n",
       "      <th>sentences</th>\n",
       "      <th>text</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>05 Ekim 2015 Pazartesi</td>\n",
       "      <td>akşam</td>\n",
       "      <td>ulusal</td>\n",
       "      <td>sahte polislerin kuryesi yakalandı</td>\n",
       "      <td>sahte polislerin kuryesi yakalandı antalya'da ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[sahte polislerin kuryesi yakalandı antalyada ...</td>\n",
       "      <td>sahte polislerin kuryesi yakalandı sahte polis...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11 Eylül 2015 Cuma</td>\n",
       "      <td>akşam</td>\n",
       "      <td>ulusal</td>\n",
       "      <td>kürt üz ama hain değiliz</td>\n",
       "      <td>kürt'üz ama hain değiliz suriye sınırında devr...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[kürtüz ama hain değiliz suriye sınırında devr...</td>\n",
       "      <td>kürt üz ama hain değiliz kürtüz ama hain değil...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>25 Eylül 2015 Cuma</td>\n",
       "      <td>akşam</td>\n",
       "      <td>ulusal</td>\n",
       "      <td>suriyeli gelinden altın vurgunu</td>\n",
       "      <td>kuyumcuda altın alırken fotoğraf çektirdi. sur...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[kuyumcuda altın alırken fotoğraf çektirdi, su...</td>\n",
       "      <td>suriyeli gelinden altın vurgunu kuyumcuda altı...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>07 Eylül 2015 Pazartesi</td>\n",
       "      <td>anayurt</td>\n",
       "      <td>ulusal</td>\n",
       "      <td>mustafa nevruz sınacı</td>\n",
       "      <td>mustafa nevruz sınacı lgercek. abd'li yahudi, ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[mustafa nevruz sınacı lgercek, abdli yahudi b...</td>\n",
       "      <td>mustafa nevruz sınacı mustafa nevruz sınacı lg...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>21 Eylül 2015 Pazartesi</td>\n",
       "      <td>anayurt</td>\n",
       "      <td>ulusal</td>\n",
       "      <td>mustafa nevruz sınacı</td>\n",
       "      <td>mustafa nevruz sınacı yazıyor, gercek. abd'li ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[mustafa nevruz sınacı yazıyor gercek, abdli y...</td>\n",
       "      <td>mustafa nevruz sınacı mustafa nevruz sınacı ya...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25010</th>\n",
       "      <td>25061</td>\n",
       "      <td>02 Mayıs 2014 Cuma</td>\n",
       "      <td>yeni asya</td>\n",
       "      <td>hepsi</td>\n",
       "      <td>amnesty ınternational ve global ahlaksızlık</td>\n",
       "      <td>doğu veya batı s. bulut@saidnursi. de amnesty ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[doğu veya batı, de amnesty ınternational ve g...</td>\n",
       "      <td>amnesty ınternational ve global ahlaksızlık do...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25011</th>\n",
       "      <td>25062</td>\n",
       "      <td>26 Mart 2014 Çarşamba</td>\n",
       "      <td>yeni konya</td>\n",
       "      <td>hepsi</td>\n",
       "      <td>çanakkale asla unutulmamalı llnutturulmamalı</td>\n",
       "      <td>çanakkale, asla unutulmamalı, llnutturulmamalı...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[çanakkale asla unutulmamalı llnutturulmamalı ...</td>\n",
       "      <td>çanakkale asla unutulmamalı llnutturulmamalı ç...</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25012</th>\n",
       "      <td>25063</td>\n",
       "      <td>04 Nisan 2014 Cuma</td>\n",
       "      <td>yeni mesaj</td>\n",
       "      <td>hepsi</td>\n",
       "      <td>sömürü projesi olarak bop</td>\n",
       "      <td>btp genel başkanı prof. dr. haydar bas ın kale...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[btp genel başkanı prof, haydar bas ın kalemin...</td>\n",
       "      <td>sömürü projesi olarak bop btp genel başkanı pr...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25013</th>\n",
       "      <td>25064</td>\n",
       "      <td>24 Şubat 2014 Pazartesi</td>\n",
       "      <td>yeni mesaj</td>\n",
       "      <td>hepsi</td>\n",
       "      <td>doğruluş zeminimiz helali bir millet istiklali...</td>\n",
       "      <td>prof. dr. nurullah çetin doğruluş zeminimiz: '...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[nurullah çetin doğruluş zeminimiz helali bir ...</td>\n",
       "      <td>doğruluş zeminimiz helali bir millet istiklali...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25014</th>\n",
       "      <td>25065</td>\n",
       "      <td>17 Ocak 2014 Cuma</td>\n",
       "      <td>çorum hakimiyet</td>\n",
       "      <td>hepsi</td>\n",
       "      <td>yahudilikten islama yönelen bir sahabi abdulla...</td>\n",
       "      <td>yahudilikten islam'a yönelen bir sahabi abdull...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[yahudilikten islama yönelen bir sahabi abdull...</td>\n",
       "      <td>yahudilikten islama yönelen bir sahabi abdulla...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25015 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                     date         pub_name    type  \\\n",
       "0          0   05 Ekim 2015 Pazartesi            akşam  ulusal   \n",
       "1          1       11 Eylül 2015 Cuma            akşam  ulusal   \n",
       "2          2       25 Eylül 2015 Cuma            akşam  ulusal   \n",
       "3          3  07 Eylül 2015 Pazartesi          anayurt  ulusal   \n",
       "4          4  21 Eylül 2015 Pazartesi          anayurt  ulusal   \n",
       "...      ...                      ...              ...     ...   \n",
       "25010  25061       02 Mayıs 2014 Cuma        yeni asya   hepsi   \n",
       "25011  25062    26 Mart 2014 Çarşamba       yeni konya   hepsi   \n",
       "25012  25063       04 Nisan 2014 Cuma       yeni mesaj   hepsi   \n",
       "25013  25064  24 Şubat 2014 Pazartesi       yeni mesaj   hepsi   \n",
       "25014  25065        17 Ocak 2014 Cuma  çorum hakimiyet   hepsi   \n",
       "\n",
       "                                                   title  \\\n",
       "0                     sahte polislerin kuryesi yakalandı   \n",
       "1                               kürt üz ama hain değiliz   \n",
       "2                        suriyeli gelinden altın vurgunu   \n",
       "3                                  mustafa nevruz sınacı   \n",
       "4                                  mustafa nevruz sınacı   \n",
       "...                                                  ...   \n",
       "25010        amnesty ınternational ve global ahlaksızlık   \n",
       "25011       çanakkale asla unutulmamalı llnutturulmamalı   \n",
       "25012                          sömürü projesi olarak bop   \n",
       "25013  doğruluş zeminimiz helali bir millet istiklali...   \n",
       "25014  yahudilikten islama yönelen bir sahabi abdulla...   \n",
       "\n",
       "                                                 content Label  \\\n",
       "0      sahte polislerin kuryesi yakalandı antalya'da ...  hate   \n",
       "1      kürt'üz ama hain değiliz suriye sınırında devr...  hate   \n",
       "2      kuyumcuda altın alırken fotoğraf çektirdi. sur...  hate   \n",
       "3      mustafa nevruz sınacı lgercek. abd'li yahudi, ...  hate   \n",
       "4      mustafa nevruz sınacı yazıyor, gercek. abd'li ...  hate   \n",
       "...                                                  ...   ...   \n",
       "25010  doğu veya batı s. bulut@saidnursi. de amnesty ...  hate   \n",
       "25011  çanakkale, asla unutulmamalı, llnutturulmamalı...  hate   \n",
       "25012  btp genel başkanı prof. dr. haydar bas ın kale...  hate   \n",
       "25013  prof. dr. nurullah çetin doğruluş zeminimiz: '...  hate   \n",
       "25014  yahudilikten islam'a yönelen bir sahabi abdull...  hate   \n",
       "\n",
       "                                               sentences  \\\n",
       "0      [sahte polislerin kuryesi yakalandı antalyada ...   \n",
       "1      [kürtüz ama hain değiliz suriye sınırında devr...   \n",
       "2      [kuyumcuda altın alırken fotoğraf çektirdi, su...   \n",
       "3      [mustafa nevruz sınacı lgercek, abdli yahudi b...   \n",
       "4      [mustafa nevruz sınacı yazıyor gercek, abdli y...   \n",
       "...                                                  ...   \n",
       "25010  [doğu veya batı, de amnesty ınternational ve g...   \n",
       "25011  [çanakkale asla unutulmamalı llnutturulmamalı ...   \n",
       "25012  [btp genel başkanı prof, haydar bas ın kalemin...   \n",
       "25013  [nurullah çetin doğruluş zeminimiz helali bir ...   \n",
       "25014  [yahudilikten islama yönelen bir sahabi abdull...   \n",
       "\n",
       "                                                    text  phase  \n",
       "0      sahte polislerin kuryesi yakalandı sahte polis...  train  \n",
       "1      kürt üz ama hain değiliz kürtüz ama hain değil...   test  \n",
       "2      suriyeli gelinden altın vurgunu kuyumcuda altı...   test  \n",
       "3      mustafa nevruz sınacı mustafa nevruz sınacı lg...  train  \n",
       "4      mustafa nevruz sınacı mustafa nevruz sınacı ya...  train  \n",
       "...                                                  ...    ...  \n",
       "25010  amnesty ınternational ve global ahlaksızlık do...  train  \n",
       "25011  çanakkale asla unutulmamalı llnutturulmamalı ç...    val  \n",
       "25012  sömürü projesi olarak bop btp genel başkanı pr...  train  \n",
       "25013  doğruluş zeminimiz helali bir millet istiklali...  train  \n",
       "25014  yahudilikten islama yönelen bir sahabi abdulla...  train  \n",
       "\n",
       "[25015 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8cf8e2f-c6ff-4d12-9164-ed5dcd5c3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04afaee0-c305-4900-b852-6c101eb53c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDVDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, idxs):\n",
    "        self.label_encodings = {\"not_hate\": 0, \"hate\": 1}\n",
    "        self.encodings = encodings\n",
    "        self.labels = [self.label_encodings[label] for label in labels]\n",
    "        self.idxs = idxs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4951616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, train_texts, train_labels = list(data.loc[data[\"phase\"] == \"train\", \"id\"].values), list(data.loc[data[\"phase\"] == \"train\", \"text\"].values), list(data.loc[data[\"phase\"] == \"train\", \"Label\"].values)\n",
    "val_idxs, val_texts, val_labels = list(data.loc[data[\"phase\"] == \"val\", \"id\"].values), list(data.loc[data[\"phase\"] == \"val\", \"text\"].values), list(data.loc[data[\"phase\"] == \"val\", \"Label\"].values)\n",
    "test_idxs, test_texts, test_labels = list(data.loc[data[\"phase\"] == \"test\", \"id\"].values), list(data.loc[data[\"phase\"] == \"test\", \"text\"].values), list(data.loc[data[\"phase\"] == \"test\", \"Label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9985b2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dist: Counter({'hate': 10074, 'not_hate': 9944})\n",
      "Validation dist: Counter({'hate': 1257, 'not_hate': 1243})\n",
      "Test dist: Counter({'hate': 1255, 'not_hate': 1242})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dist: {Counter(train_labels)}\")\n",
    "print(f\"Validation dist: {Counter(val_labels)}\")\n",
    "print(f\"Test dist: {Counter(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8afb469-387c-483a-8501-0212ec785468",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55d81c32-7f3e-40d9-8760-4e552000f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90f16f64-359d-4a7c-a092-141800b08a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HDVDataset(train_encodings, train_labels, train_idxs)\n",
    "val_dataset = HDVDataset(val_encodings, val_labels, val_idxs)\n",
    "test_dataset = HDVDataset(test_encodings, test_labels, test_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c599074-ae92-44fb-8af5-fe40c98255f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99600c4f-63ba-490e-a4bc-4824caeb784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 7.55kB [00:00, 1.62MB/s]                   \n",
      "Downloading builder script: 7.38kB [00:00, 1.53MB/s]                   \n",
      "Downloading builder script: 4.21kB [00:00, 1.07MB/s]                   \n",
      "Downloading builder script: 6.50kB [00:00, 5.02MB/s]                   \n"
     ]
    }
   ],
   "source": [
    "prec = load_metric(\"precision\")\n",
    "rec = load_metric(\"recall\")\n",
    "acc = load_metric(\"accuracy\")\n",
    "f1 = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    result = {}\n",
    "    for mtrc in [prec, rec, acc, f1]:\n",
    "        mtrc_result = mtrc.compute(predictions=predictions, references=labels)\n",
    "        result.update(mtrc_result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b0cc9-5464-4f6e-9350-9762310187cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Huggingface models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0d19972-4ecd-4b3d-9630-b69ea9a70f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=hdv_hate_speech\n",
      "env: WANDB_LOG_MODEL=true\n",
      "env: WANDB_WATCH=all\n",
      "env: WANDB_NOTEBOOK_NAME=berturk\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=hdv_hate_speech\n",
    "%env WANDB_LOG_MODEL=true\n",
    "%env WANDB_WATCH=all\n",
    "%env WANDB_NOTEBOOK_NAME=berturk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b920fcab-f99b-4000-8550-ed709ee3f11a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import wandb\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72bd936e-59fa-4392-bda8-1f9283888e45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find berturk.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnlpboun\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36d9fba1-cc80-4d54-9fcc-2a484dccfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"../experiments/results/\"\n",
    "logs_path = \"../experiments/logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc90eea2-815f-41d6-8952-ce0ad236515d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 706M/706M [03:47<00:00, 3.26MB/s] \n",
      "Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94fd1d23-3c14-4fcd-8668-28a3fc8b7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(results_path, \"berturk_128K_arute\"),               # output directory\n",
    "    num_train_epochs=2,                                                  # total number of training epochs\n",
    "    per_device_train_batch_size=4,                                       # batch size per device during training\n",
    "    per_device_eval_batch_size=4,                                        # batch size for evaluation\n",
    "    warmup_steps=500,                                                    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                                                   # strength of weight decay\n",
    "    logging_dir=os.path.join(results_path, \"berturk_128K_arute\"),              # directory for storing logs\n",
    "    logging_steps=20,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    save_steps=1000,\n",
    "    learning_rate=1e-05,\n",
    "    run_name=\"berturk_128K_uncased_lre-5\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                                                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                                                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,                                         # training dataset\n",
    "    eval_dataset=val_dataset,                                            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e80a4c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arute/miniconda3/envs/nlp_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20018\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1570' max='10010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1570/10010 03:59 < 21:28, 6.55 it/s, Epoch 0.31/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.575400</td>\n",
       "      <td>0.470825</td>\n",
       "      <td>0.778609</td>\n",
       "      <td>0.828162</td>\n",
       "      <td>0.795200</td>\n",
       "      <td>0.802621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.326600</td>\n",
       "      <td>0.417024</td>\n",
       "      <td>0.866092</td>\n",
       "      <td>0.879873</td>\n",
       "      <td>0.871200</td>\n",
       "      <td>0.872928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.452083</td>\n",
       "      <td>0.893755</td>\n",
       "      <td>0.876691</td>\n",
       "      <td>0.885600</td>\n",
       "      <td>0.885141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K_arute/checkpoint-1000\n",
      "Configuration saved in ../experiments/results/berturk_128K_arute/checkpoint-1000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K_arute/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/site-packages/transformers/trainer.py:1424\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1422\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 1424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1425\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1428\u001b[0m ):\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c410cbd2-5664-4938-a97e-bf879f92293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tabilab/anaconda3/envs/hate_env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 20052\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10026\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tabilab/projects/notebooks/wandb/run-20220414_135711-3vbi04su</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlpboun/hdv_hate_speech/runs/3vbi04su\" target=\"_blank\">berturk_128K_uncased_lre-5</a></strong> to <a href=\"https://wandb.ai/nlpboun/hdv_hate_speech\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10026' max='10026' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10026/10026 29:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.470382</td>\n",
       "      <td>0.742204</td>\n",
       "      <td>0.847310</td>\n",
       "      <td>0.774631</td>\n",
       "      <td>0.791282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.412100</td>\n",
       "      <td>0.487355</td>\n",
       "      <td>0.869938</td>\n",
       "      <td>0.883703</td>\n",
       "      <td>0.874751</td>\n",
       "      <td>0.876766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.880620</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>0.887515</td>\n",
       "      <td>0.889585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.537385</td>\n",
       "      <td>0.930530</td>\n",
       "      <td>0.805380</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>0.863444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.674700</td>\n",
       "      <td>0.489855</td>\n",
       "      <td>0.929856</td>\n",
       "      <td>0.818038</td>\n",
       "      <td>0.877144</td>\n",
       "      <td>0.870370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.378214</td>\n",
       "      <td>0.902478</td>\n",
       "      <td>0.893196</td>\n",
       "      <td>0.897487</td>\n",
       "      <td>0.897813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.367395</td>\n",
       "      <td>0.888298</td>\n",
       "      <td>0.924842</td>\n",
       "      <td>0.903470</td>\n",
       "      <td>0.906202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.372100</td>\n",
       "      <td>0.365355</td>\n",
       "      <td>0.894453</td>\n",
       "      <td>0.918513</td>\n",
       "      <td>0.904268</td>\n",
       "      <td>0.906323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.552500</td>\n",
       "      <td>0.460086</td>\n",
       "      <td>0.850035</td>\n",
       "      <td>0.959652</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>0.901524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.378452</td>\n",
       "      <td>0.916599</td>\n",
       "      <td>0.895570</td>\n",
       "      <td>0.906262</td>\n",
       "      <td>0.905962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.355999</td>\n",
       "      <td>0.912837</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.911448</td>\n",
       "      <td>0.912114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.388397</td>\n",
       "      <td>0.903502</td>\n",
       "      <td>0.918513</td>\n",
       "      <td>0.909454</td>\n",
       "      <td>0.910945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.430900</td>\n",
       "      <td>0.378103</td>\n",
       "      <td>0.898006</td>\n",
       "      <td>0.926424</td>\n",
       "      <td>0.909852</td>\n",
       "      <td>0.911994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.479300</td>\n",
       "      <td>0.377564</td>\n",
       "      <td>0.918285</td>\n",
       "      <td>0.897943</td>\n",
       "      <td>0.908257</td>\n",
       "      <td>0.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.251800</td>\n",
       "      <td>0.366917</td>\n",
       "      <td>0.887976</td>\n",
       "      <td>0.940665</td>\n",
       "      <td>0.910251</td>\n",
       "      <td>0.913561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.256600</td>\n",
       "      <td>0.377305</td>\n",
       "      <td>0.883309</td>\n",
       "      <td>0.946203</td>\n",
       "      <td>0.909852</td>\n",
       "      <td>0.913675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.361631</td>\n",
       "      <td>0.908949</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.915038</td>\n",
       "      <td>0.916438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.391033</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.925633</td>\n",
       "      <td>0.912246</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.371968</td>\n",
       "      <td>0.913590</td>\n",
       "      <td>0.920095</td>\n",
       "      <td>0.915836</td>\n",
       "      <td>0.916831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.367300</td>\n",
       "      <td>0.364992</td>\n",
       "      <td>0.901602</td>\n",
       "      <td>0.935127</td>\n",
       "      <td>0.915836</td>\n",
       "      <td>0.918058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-1000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-1000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-2000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-2000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-3000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-3000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-4000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-4000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-4000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-5000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-5000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-5000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-6000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-6000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-6000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-7000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-7000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-7000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-8000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-8000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-8000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-9000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-9000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-9000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ../experiments/results/berturk_128K/checkpoint-10000\n",
      "Configuration saved in ../experiments/results/berturk_128K/checkpoint-10000/config.json\n",
      "Model weights saved in ../experiments/results/berturk_128K/checkpoint-10000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../experiments/results/berturk_128K/checkpoint-10000 (score: 0.3649916350841522).\n",
      "Saving model checkpoint to /tmp/tmpu3xrqjht\n",
      "Configuration saved in /tmp/tmpu3xrqjht/config.json\n",
      "Model weights saved in /tmp/tmpu3xrqjht/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10026, training_loss=0.38614428408388934, metrics={'train_runtime': 1770.0672, 'train_samples_per_second': 22.657, 'train_steps_per_second': 5.664, 'total_flos': 1.055180576415744e+16, 'train_loss': 0.38614428408388934, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad39b6-4966-4c95-aec6-669ed22de961",
   "metadata": {},
   "source": [
    "## Evaluation (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3110b46c-0a69-4358-9ddf-380376b94000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2642' max='627' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [627/627 35:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3649916350841522,\n",
       " 'eval_precision': 0.9016018306636155,\n",
       " 'eval_recall': 0.935126582278481,\n",
       " 'eval_accuracy': 0.9158356601515756,\n",
       " 'eval_f1': 0.9180582524271845,\n",
       " 'eval_runtime': 22.3554,\n",
       " 'eval_samples_per_second': 112.143,\n",
       " 'eval_steps_per_second': 28.047,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = trainer.evaluate(val_dataset)\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8703b-9124-4c17-b393-218a9faa0524",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8421fb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2497\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [0 1 1 ... 1 1 1]\n",
      " GT's: [1 1 1 ... 1 1 1]\n",
      "{'test_loss': 0.4906734824180603, 'test_precision': 0.8367931281317108, 'test_recall': 0.9314741035856574, 'test_accuracy': 0.8742490989187024, 'test_f1': 0.8815987933634993, 'test_runtime': 17.5851, 'test_samples_per_second': 141.996, 'test_steps_per_second': 35.542}\n"
     ]
    }
   ],
   "source": [
    "preds_dict = trainer.predict(test_dataset)\n",
    "predictions = preds_dict.predictions\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print(f\"Preds: {predictions}\\n GT's: {preds_dict.label_ids}\")\n",
    "print(preds_dict.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20e05409-016d-43bb-b5cc-ad44ec3cace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDVDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts_idxs, labels, tokenizer):\n",
    "        self.label_encodings = {\"not_hate\": 0, \"hate\": 1}\n",
    "        self.rev_label_encodings = {0: \"not_hate\", 1: \"hate\"}\n",
    "        \n",
    "        self.texts, self.idxs = list(np.array(texts_idxs)[:, 0]), list(np.array(texts_idxs)[:, 1])\n",
    "        self.encodings = tokenizer(self.texts, truncation=True, padding=True)\n",
    "        self.labels = [self.label_encodings[label] for label in labels]\n",
    "        self.preds = []\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _get_preds_with_idx(self):\n",
    "        df_preds = pd.DataFrame(data={\"idx\": self.idxs, \"prediction\": self.preds})\n",
    "        df_preds[\"prediction\"] = df_preds[\"prediction\"].map(self.rev_label_encodings)\n",
    "        return df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b33ce1fa-1894-4280-8ce7-960e0da33ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_idxs, labels = list(data[[\"text\", \"id\"]].values), list(data[\"Label\"].values)\n",
    "train_texts_idxs_2, val_texts_idxs_2, train_labels_2, val_labels_2 = train_test_split(texts_idxs, labels, stratify=labels, test_size=.2, shuffle=True, random_state=17)\n",
    "val_texts_idxs_2, test_texts_idxs_2, val_labels_2, test_labels_2 = train_test_split(val_texts_idxs_2, val_labels_2, stratify=val_labels_2, test_size=.5, shuffle=True, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f944246f-86a6-4010-966d-08e99938b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_idx = HDVDatasetTest(test_texts_idxs_2, test_labels_2, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa51f1-9524-47db-8249-b6d24dddccdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7aa375aa-f4f4-4478-9b29-f0f4e68c527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1 0 1 ... 1 0 1]\n",
      " GT's: [1 0 1 ... 1 0 1]\n",
      "{'test_loss': 0.42105528712272644, 'test_precision': 0.8891419893697798, 'test_recall': 0.9271575613618369, 'test_accuracy': 0.9050658157159952, 'test_f1': 0.9077519379844963, 'test_runtime': 23.0777, 'test_samples_per_second': 108.633, 'test_steps_per_second': 27.169}\n"
     ]
    }
   ],
   "source": [
    "preds_dict = trainer.predict(test_dataset)\n",
    "predictions = preds_dict.predictions\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print(f\"Preds: {predictions}\\n GT's: {preds_dict.label_ids}\")\n",
    "print(preds_dict.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "209c979d-16db-49e2-b5fa-97f4e345ca5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37011e44-0fdd-492e-9ad8-0ca38221b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2507\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1 0 1 ... 1 0 1]\n",
      " GT's: [1 0 1 ... 1 0 1]\n",
      "{'test_loss': 0.42105528712272644, 'test_precision': 0.8891419893697798, 'test_recall': 0.9271575613618369, 'test_accuracy': 0.9050658157159952, 'test_f1': 0.9077519379844963, 'test_runtime': 23.2055, 'test_samples_per_second': 108.035, 'test_steps_per_second': 27.019}\n"
     ]
    }
   ],
   "source": [
    "preds_dict_2 = trainer.predict(test_dataset_idx)\n",
    "predictions_2 = preds_dict_2.predictions\n",
    "predictions_2 = np.argmax(predictions_2, axis=1)\n",
    "print(f\"Preds: {predictions_2}\\n GT's: {preds_dict_2.label_ids}\")\n",
    "print(preds_dict_2.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2585239a-7278-4a05-ad6d-5c79efe0fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_idx.preds = predictions_2\n",
    "df_preds = test_dataset_idx._get_preds_with_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d82d23b7-f456-4230-b3cd-79d07986bcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3973</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16849</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9186</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3072</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10346</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>3672</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>18446</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>3657</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>19535</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>8945</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2507 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx prediction\n",
       "0      3973       hate\n",
       "1     16849   not_hate\n",
       "2      9186       hate\n",
       "3      3072       hate\n",
       "4     10346       hate\n",
       "...     ...        ...\n",
       "2502   3672       hate\n",
       "2503  18446   not_hate\n",
       "2504   3657       hate\n",
       "2505  19535   not_hate\n",
       "2506   8945       hate\n",
       "\n",
       "[2507 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab619e57-c533-480e-bc2a-c32d0f174665",
   "metadata": {},
   "source": [
    "## Report to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbd5a102-9b0d-45be-9335-3fa46f999bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_preds = pd.merge(data, df_preds, left_on=\"id\", right_on=\"idx\", how=\"right\").drop(\"idx\", axis=1)\n",
    "df_label_preds.to_excel(\"../outputs/labels_preds_berturk_2022-04-14.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce0d0553-4d65-455f-bd9a-41a2c41b013a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3973</td>\n",
       "      <td>haber seyfullah koyuncu freddy mercurynin aske...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16849</td>\n",
       "      <td>kilisede hz fatıma nın doğumu kutlandı kilised...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9186</td>\n",
       "      <td>yunanlıların verdiği zararları anlatan resmi d...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3072</td>\n",
       "      <td>itı hr it serdar çalışkan yaptığı açıklamada ş...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10346</td>\n",
       "      <td>mültecileri dövüp geri gönderdiler mültecileri...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>3672</td>\n",
       "      <td>saitiyor su rl yıu ur bpfjmkmmiami rj fiil il ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>18446</td>\n",
       "      <td>bu işbirllfii türkiye ye örnek oucak bu işbirl...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>3657</td>\n",
       "      <td>mersinde işlenen cinayetle ilgili suriyeli tut...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>19535</td>\n",
       "      <td>siparişle kurulan proje örgütlerdir siparişle ...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>8945</td>\n",
       "      <td>içimizi içimizi döktüğümüz duvarlar handan yal...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2507 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text     Label  \\\n",
       "0      3973  haber seyfullah koyuncu freddy mercurynin aske...      hate   \n",
       "1     16849  kilisede hz fatıma nın doğumu kutlandı kilised...  not_hate   \n",
       "2      9186  yunanlıların verdiği zararları anlatan resmi d...      hate   \n",
       "3      3072  itı hr it serdar çalışkan yaptığı açıklamada ş...      hate   \n",
       "4     10346  mültecileri dövüp geri gönderdiler mültecileri...      hate   \n",
       "...     ...                                                ...       ...   \n",
       "2502   3672  saitiyor su rl yıu ur bpfjmkmmiami rj fiil il ...      hate   \n",
       "2503  18446  bu işbirllfii türkiye ye örnek oucak bu işbirl...  not_hate   \n",
       "2504   3657  mersinde işlenen cinayetle ilgili suriyeli tut...      hate   \n",
       "2505  19535  siparişle kurulan proje örgütlerdir siparişle ...  not_hate   \n",
       "2506   8945  içimizi içimizi döktüğümüz duvarlar handan yal...      hate   \n",
       "\n",
       "     prediction  \n",
       "0          hate  \n",
       "1      not_hate  \n",
       "2          hate  \n",
       "3          hate  \n",
       "4          hate  \n",
       "...         ...  \n",
       "2502       hate  \n",
       "2503   not_hate  \n",
       "2504       hate  \n",
       "2505   not_hate  \n",
       "2506       hate  \n",
       "\n",
       "[2507 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a81a4df-ebec-4a0c-b12e-dee52fdd9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"../data/data_cleaned_sentences_2020-04-10.csv\", sep='|', converters={'sentences': pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ec565bc-ab4f-4756-ab2e-4d78bc1bb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = data_raw.reset_index().rename(columns={\"index\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c3c1a31-0846-4d15-9eb9-e902eeae20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_and_preds = pd.merge(data_raw, df_preds, left_on=\"id\", right_on=\"idx\", how=\"right\").drop(\"idx\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d26d628d-4c80-4ce2-87d3-e531957336a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_and_preds.to_excel(\"../outputs/data_and_preds_2022-04-14.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9e89c12-ec6d-4715-af9b-f77f0db75658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9050658157159952"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_data_and_preds[\"Label\"] == df_data_and_preds[\"prediction\"]) / df_data_and_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c4ff115a-25c0-41cd-9a99-9eb2b5c187c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hate': 1264, 'not_hate': 1243})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828fbda-52e1-47aa-a037-495319d60750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea187edb-7a5d-4522-ada6-506674386429",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../outputs/data_and_preds_2022-04-14.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0036d72-5356-426b-b367-3a6202140069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9046856227472968"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(df[\"Label\"] == df[\"prediction\"]) - len(remove_id_from_test)) / (df.shape[0] - len(remove_id_from_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d46c4-323b-4151-b093-920fc9a7609f",
   "metadata": {},
   "source": [
    "## Duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291e06b8-5b3a-4d4c-be19-8c0a104972f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_excel = pd.read_excel(\"../outputs/data_and_preds_2022-04-14.xlsx\")[\"id\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16084c1-14aa-4787-b137-3fcffd686f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.array(test_texts_idxs_2)[:, 1]) == id_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5234732-8ef7-4d34-9041-564f3570a220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set.intersection(set(np.array(test_texts_idxs_2)[:, 1]), set(np.array(train_texts_idxs_2)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f10f8-ebdf-47a0-b5c3-2eef44886513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8eafbcf3-f78e-430a-abaf-bced5e0f70d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pair = data[data[[\"text\"]].duplicated(keep=\"first\")].sort_values(\"text\").id.values.tolist()\n",
    "last_pair = data[data[[\"text\"]].duplicated(keep=\"last\")].sort_values(\"text\").id.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7831696a-0bd6-477a-998a-43b6d53b193a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [[first_pair[i], last_pair[i]] for i in range(len(first_pair))]\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99e7e8fc-a3b0-4d8d-9078-33117af2fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, val_idxs, test_idxs = list(np.array(train_texts_idxs_2)[:, 1]), list(np.array(val_texts_idxs_2)[:, 1]), list(np.array(test_texts_idxs_2)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22b1a03d-ff28-4c96-ae68-eb46c7a7de37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_duplicate_rows(train_idxs, test_idxs, pairs):\n",
    "    remove_id_from_test = []\n",
    "    for pair in pairs:\n",
    "        if pair[0] in train_idxs and pair[1] in train_idxs:\n",
    "            print(pair, \" is all in train set!\")\n",
    "        elif pair[0] in train_idxs and pair[1] in test_idxs:\n",
    "            print(f\"{pair[0]} in train, {pair[1]} in test, please delete from test!!!\")\n",
    "            remove_id_from_test.append(pair[1])\n",
    "        elif pair[1] in train_idxs and pair[0] in test_idxs:\n",
    "            print(f\"{pair[1]} in train, {pair[0]} in test, please delete from test!!!\")\n",
    "            remove_id_from_test.append(pair[0])\n",
    "        elif pair[0] in test_idxs and pair[1] in test_idxs:\n",
    "            print(f\"{pair[0]} in test, {pair[1]} in test, please delete one of them from test!!!\")\n",
    "            remove_id_from_test.append(pair[1])\n",
    "    remove_test_indices = []\n",
    "    for remove_id in remove_id_from_test:\n",
    "        remove_test_indices.append(test_idxs.index(remove_id))\n",
    "    remove_test_indices\n",
    "    return remove_id_from_test, remove_test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e138719f-89c6-47e0-9b25-9d34c9c61493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_id_from_test = detect_duplicate_rows(train_idxs, test_idxs, pairs)\n",
    "remove_id_from_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4ab89b3-c3ad-412b-bedf-d5b497cb6df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_id_from_val = detect_duplicate_rows(train_idxs, val_idxs, pairs)\n",
    "remove_id_from_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5fd6ab-6007-43db-8423-10ac8af732fe",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa3e05c3-543c-439c-b544-ac099672bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"../experiments/results/berturk_128K/checkpoint-10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52c3c2d3-765e-4610-98df-7f4683585bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDVDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts_idxs, labels, tokenizer, remove_idxs):\n",
    "        self.label_encodings = {\"not_hate\": 0, \"hate\": 1}\n",
    "        self.rev_label_encodings = {0: \"not_hate\", 1: \"hate\"}\n",
    "        \n",
    "        self.texts, self.idxs = list(np.array(texts_idxs)[:, 0]), list(np.array(texts_idxs)[:, 1])\n",
    "        print(len(self.idxs))\n",
    "        self.texts = [text for i, text in enumerate(self.texts) if i not in remove_idxs]\n",
    "        self.idxs = [idx for i, idx in enumerate(self.idxs) if i not in remove_idxs]\n",
    "        self.encodings = tokenizer(self.texts, truncation=True, padding=True)\n",
    "        self.labels = [self.label_encodings[label] for i, label in enumerate(labels) if i not in remove_idxs]\n",
    "        self.preds = []\n",
    "        print(len(self.idxs))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _get_preds_with_idx(self):\n",
    "        df_preds = pd.DataFrame(data={\"idx\": self.idxs, \"prediction\": self.preds})\n",
    "        df_preds[\"prediction\"] = df_preds[\"prediction\"].map(self.rev_label_encodings)\n",
    "        return df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3df4d9c6-04e3-430f-9b02-a2e6c5e21349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2507\n",
      "2497\n"
     ]
    }
   ],
   "source": [
    "test_dataset_idx_cleaned = HDVDatasetTest(test_texts_idxs_2, test_labels_2, tokenizer, remove_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e4c2db2-0a4a-4865-9981-fc789fcb3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                                                         # the instantiated 🤗 Transformers model to be trained\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "542b9347-3921-44e9-8cab-ed99cb243699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2497\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1 0 1 ... 1 0 1]\n",
      " GT's: [1 0 1 ... 1 0 1]\n",
      "{'test_loss': 0.4224632978439331, 'test_precision': 0.8884644766997708, 'test_recall': 0.9266932270916335, 'test_accuracy': 0.9046856227472968, 'test_f1': 0.9071762870514819, 'test_runtime': 17.3161, 'test_samples_per_second': 144.201, 'test_steps_per_second': 18.076}\n"
     ]
    }
   ],
   "source": [
    "preds_dict_3 = trainer.predict(test_dataset_idx_cleaned)\n",
    "predictions_3 = preds_dict_3.predictions\n",
    "predictions_3 = np.argmax(predictions_3, axis=1)\n",
    "print(f\"Preds: {predictions_3}\\n GT's: {preds_dict_3.label_ids}\")\n",
    "print(preds_dict_3.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf03f58-29f3-4fd2-9000-a3b62809d059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP Env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
